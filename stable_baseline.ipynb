{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_stack_obj_trj(obs, inpt):\n",
    "    #inpt = N,L,D\n",
    "    embed_size = obs.size(-1)\n",
    "    if type(inpt) is torch.Tensor:\n",
    "        result = inpt\n",
    "    else:\n",
    "        result = torch.zeros(size=inpt, device=obs.device)\n",
    "    print(embed_size)\n",
    "    print(obsv.size())\n",
    "    print(result.size())\n",
    "    print(result[:,-1,:-2].size())\n",
    "    result[:,-1,:-embed_size] = obs\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 3, 6])\n",
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [2, 2].  Tensor sizes: [2, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000033?line=0'>1</a>\u001b[0m obsv \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\u001b[39m4\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mrepeat((\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000033?line=1'>2</a>\u001b[0m req_size \u001b[39m=\u001b[39m (obsv\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m3\u001b[39m, obsv\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000033?line=2'>3</a>\u001b[0m rso \u001b[39m=\u001b[39m right_stack_obj_trj(obsv, req_size)\n",
      "\u001b[1;32m/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb Cell 2'\u001b[0m in \u001b[0;36mright_stack_obj_trj\u001b[0;34m(obs, inpt)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000032?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(result\u001b[39m.\u001b[39msize())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000032?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(result[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:\u001b[39m-\u001b[39membed_size]\u001b[39m.\u001b[39msize())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000032?line=11'>12</a>\u001b[0m result[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:\u001b[39m-\u001b[39membed_size] \u001b[39m=\u001b[39m obs\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000032?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [2, 2].  Tensor sizes: [2, 4]"
     ]
    }
   ],
   "source": [
    "obsv = torch.arange(4).unsqueeze(0).repeat((2,1))\n",
    "req_size = (obsv.size(0), 3, obsv.size(-1)+2)\n",
    "rso = right_stack_obj_trj(obsv, req_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '/home/hendrik/Documents/master_project/LokalData/metaworld/small/train/'\n",
    "path_validate = '/home/hendrik/Documents/master_project/LokalData/metaworld/small/val/'\n",
    "train_data = TorchDatasetMWToy(path=path_train, device='cpu')\n",
    "val_data = TorchDatasetMWToy(path=path_validate, device='cpu')\n",
    "print(train_data.data.shape)\n",
    "print(train_data.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global SAMPLED_ENVS\n",
    "global STEPS_TAKEN\n",
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_env():\n",
    "    def __init__(self, data=None):\n",
    "        if data is None:\n",
    "            data = train_data\n",
    "        #obs = step, data, action, current_env\n",
    "        self.observation_space = gym.spaces.box.Box(np.array([0, -2,-2,-2,-2, 0,0,0,0.,0]), np.array([6, 2,2,2,2, 1,1,1,1.,train_data.data.size(0)]), (10,), float)\n",
    "        #next state (4)\n",
    "        self.action_space = gym.spaces.box.Box(np.array([0,0,0,0]), np.array([1,1,1,1]), (4,), float)\n",
    "        self.metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}\n",
    "        self.steps = 0\n",
    "        self.current_env = -1\n",
    "        self.data = data\n",
    "        self.label = data.label\n",
    "        self.traj = None\n",
    "        self.num_envs = 1\n",
    "    def reset(self):\n",
    "        global SAMPLED_ENVS\n",
    "        global STEPS_TAKEN\n",
    "        STEPS_TAKEN += 1\n",
    "        SAMPLED_ENVS += 1\n",
    "        self.traj = None\n",
    "        self.current_env = (self.current_env + 1)%len(self.data)\n",
    "        self.steps = 0\n",
    "        last_action = torch.zeros(4, dtype=float, device=self.data.data.device)\n",
    "        step = torch.tensor(self.steps, device=self.data.data.device)\n",
    "        current_env = torch.tensor(self.current_env, device=self.data.data.device)\n",
    "        data = self.data.data[self.current_env, 0]\n",
    "        #label = self.label[self.current_env,0]\n",
    "        state = torch.cat((step.view(1), data, last_action, current_env.view(1)), dim=0).numpy()\n",
    "        #print(f'reset: {state.shape}')\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        global STEPS_TAKEN\n",
    "        STEPS_TAKEN += 1\n",
    "        if type(action) is np.ndarray:\n",
    "            action = torch.tensor(action, device=self.data.data.device)\n",
    "        if self.traj is None:\n",
    "            self.traj = action.reshape(1,-1)\n",
    "        else:\n",
    "            self.traj = torch.cat((self.traj, action.reshape(1,-1)), dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        self.steps += 1\n",
    "        step = torch.tensor(self.steps, device=self.data.data.device)\n",
    "        current_env = torch.tensor(self.current_env, device=self.data.data.device)\n",
    "\n",
    "        #label = self.label[self.current_env, self.current_step]\n",
    "        data = self.data.data[self.current_env, 0]\n",
    "\n",
    "        state = torch.cat((step.view(1), data, action.reshape(-1), current_env.view(1)), dim=0).numpy()\n",
    "        #print(f'step: {state.shape}')\n",
    "\n",
    "        if self.steps >= self.label.size(1):\n",
    "            tol_neg = -0.30*torch.ones([self.traj.size(-1)])\n",
    "            tol_pos = 0.45*torch.ones([self.traj.size(-1)])\n",
    "            reward = int(check_outpt(self.label[self.current_env].unsqueeze(0), self.traj.unsqueeze(0), tol_neg=tol_neg, tol_pos=tol_pos))\n",
    "            return (state, reward, True, {})\n",
    "        else:\n",
    "            return (state, 0., False, {})\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def render(self, mode):\n",
    "        pass\n",
    "\n",
    "class toy_exper_model(OnPolicyAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            policy: Union[str, Type[ActorCriticPolicy]] = 'MlpPolicy',\n",
    "            env: Union[GymEnv, str] = None,\n",
    "            learning_rate: Union[float, Schedule] = 3e-4,\n",
    "            n_steps: int = 2048,\n",
    "            batch_size: int = 64,\n",
    "            n_epochs: int = 10,\n",
    "            gamma: float = 0.99,\n",
    "            gae_lambda: float = 0.95,\n",
    "            clip_range: Union[float, Schedule] = 0.2,\n",
    "            clip_range_vf: Union[None, float, Schedule] = None,\n",
    "            normalize_advantage: bool = True,\n",
    "            ent_coef: float = 0.0,\n",
    "            vf_coef: float = 0.5,\n",
    "            max_grad_norm: float = 0.5,\n",
    "            use_sde: bool = False,\n",
    "            sde_sample_freq: int = -1,\n",
    "            target_kl: Optional[float] = None,\n",
    "            tensorboard_log: Optional[str] = None,\n",
    "            create_eval_env: bool = False,\n",
    "            policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            verbose: int = 0,\n",
    "            seed: Optional[int] = None,\n",
    "            device: Union[th.device, str] = \"auto\",\n",
    "            _init_setup_model: bool = True,\n",
    "            train_data = None\n",
    "        ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            create_eval_env=create_eval_env,\n",
    "            seed=seed,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "        self.data = train_data.data\n",
    "        self.label = train_data.label\n",
    "        #obs = step, data, action, current_env\n",
    "        self.observation_space = gym.spaces.box.Box(np.array([0, -2,-2,-2,-2, 0,0,0,0.,0]), np.array([6, 2,2,2,2, 1,1,1,1.,train_data.data.size(0)]), (10,), float)\n",
    "        #next state (4)\n",
    "        self.action_space = gym.spaces.box.Box(np.array([0,0,0,0]), np.array([1,1,1,1]), (4,), float)\n",
    "\n",
    "    def predict(self, obs, state=None, episode_start=None, deterministic=False):\n",
    "        step = int(obs.reshape(-1)[0])\n",
    "        env = int(obs.reshape(-1)[-1])\n",
    "        #print(f'expert: {self.label[env, step].reshape(1, -1).shape}')\n",
    "        return self.label[env, step].reshape(1, -1), self.label[env, step].reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_env = my_env(data=train_data)\n",
    "val_env = my_env(data=val_data)\n",
    "my_expert = toy_exper_model(train_data=train_data, env=toy_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions():\n",
    "    expert = my_expert\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(toy_env)]),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=10000),\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = sample_expert_transitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Simon Stepputtis <sstepput@asu.edu>, Interactive Robotics Lab, Arizona State University\n",
    "\n",
    "from pickle import NONE\n",
    "from urllib.parse import non_hierarchical\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from hashids import Hashids\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class TBoardGraphsTorch():\n",
    "    def __init__(self, logname= None, data_path = None):\n",
    "        if logname is not None:\n",
    "            self.__hashids           = Hashids()\n",
    "            #self.logdir              = \"Data/TBoardLog/\" + logname + \"/\"\n",
    "            self.logdir              = os.path.join(data_path, \"gboard/\" + logname + \"/\")\n",
    "            print(f'log dir: {self.logdir + \"train/\"}')\n",
    "            self.__tboard_train      = tf.summary.create_file_writer(self.logdir + \"train/\")\n",
    "            self.__tboard_validation = tf.summary.create_file_writer(self.logdir + \"validate/\")\n",
    "            #self.voice               = Voice(path=data_path)\n",
    "        self.fig, self.ax = plt.subplots(3,3)\n",
    "\n",
    "    def startDebugger(self):\n",
    "        tf.summary.trace_on(graph=True, profiler=True)\n",
    "    \n",
    "    def stopDebugger(self):\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=self.logdir)\n",
    "\n",
    "    def finishFigure(self, fig):\n",
    "        fig.canvas.draw()\n",
    "        data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        return data\n",
    "    \n",
    "    def addTrainScalar(self, name, value, stepid):\n",
    "        with self.__tboard_train.as_default():\n",
    "            tfvalue = self.torch2tf(value)\n",
    "            tf.summary.scalar(name, tfvalue, step=stepid)\n",
    "\n",
    "    def addValidationScalar(self, name, value, stepid):\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tfvalue = self.torch2tf(value)\n",
    "            tf.summary.scalar(name, tfvalue, step=stepid)\n",
    "\n",
    "    def torch2tf(self, inpt):\n",
    "        if inpt is not None:\n",
    "            return tf.convert_to_tensor(inpt.detach().cpu().numpy())\n",
    "        else:\n",
    "            return inpt\n",
    "\n",
    "    def plotTrajectory(self, y_true, y_pred, dt_true, dt_pred, stepid):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "        tf_dt_true = self.torch2tf(dt_true)\n",
    "        tf_dt_pred = self.torch2tf(dt_pred)\n",
    "\n",
    "        fig, ax = plt.subplots(3,3)\n",
    "        fig.set_size_inches(9, 9)\n",
    "\n",
    "        tf_dt_true = 1.0/tf_dt_true.numpy()\n",
    "        tf_dt_pred = 1.0/tf_dt_pred.numpy()[0]\n",
    "\n",
    "        max_trj_len = tf_y_true.shape[0]\n",
    "        for sp in range(7):\n",
    "            idx = sp // 3\n",
    "            idy = sp  % 3\n",
    "            ax[idx,idy].clear()\n",
    "            ax[idx,idy].plot(range(max_trj_len), tf_y_pred[:,sp], alpha=0.5, color='midnightblue')\n",
    "            ax[idx,idy].plot(range(max_trj_len), tf_y_true[:,sp], alpha=0.5, color='forestgreen')\n",
    "            # ax[idx,idy].plot([dt_pred, dt_pred], [-0.1, 1.1], alpha=0.5, linestyle=\":\", color=\"midnightblue\")\n",
    "            # ax[idx,idy].plot([dt_true, dt_true], [-0.1, 1.1], alpha=0.5, linestyle=\":\", color=\"forestgreen\")\n",
    "            # ax[idx,idy].set_ylim([-0.1, 1.1])\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Trajectory\", data=result, step=stepid)\n",
    "\n",
    "    def idToText(self, id):\n",
    "        names = [\"\", \"ysr\", \"rsr\", \"gsr\", \"bsr\", \"psr\", \"ylr\", \"rlr\", \"glr\", \"blr\", \"plr\", \"yss\", \"rss\", \"gss\", \"bss\", \"pss\", \"yls\", \"rls\", \"gls\", \"bls\", \"pls\"]\n",
    "        return names[id]\n",
    "\n",
    "    def plotImageRegions(self, image, image_dict, stepid):\n",
    "        # Visualization of the results of a detection.\n",
    "        num_detected = len([v for v in image_dict[\"detection_scores\"][0] if v > 0.5]) \n",
    "        image_np     = image.numpy()       \n",
    "        for i in range(num_detected):\n",
    "            ymin, xmin, ymax, xmax = image_dict['detection_boxes'][0][i,:]\n",
    "            pt1 = (int(xmin*image_np.shape[1]), int(ymin*image_np.shape[0]))\n",
    "            pt2 = (int(xmax*image_np.shape[1]), int(ymax*image_np.shape[0]))\n",
    "            image_np = cv2.rectangle(image_np, pt1, pt2, (255, 0, 0), 2)\n",
    "            image_np = cv2.putText(image_np, self.idToText(image_dict['detection_classes'][0][i]) + \" {:.1f}%\".format(image_dict[\"detection_scores\"][0][i] * 100), pt1, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(image_np)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Image\", data=result, step=stepid)\n",
    "\n",
    "    def plotAttention(self, attention_weights, image_dict, language, stepid):\n",
    "        tf_attention_weights = self.torch2tf(attention_weights)\n",
    "        tf_language = self.torch2tf(language)\n",
    "\n",
    "        tf_attention_weights = tf_attention_weights.numpy()\n",
    "        classes           = image_dict[\"detection_classes\"][0][:len(tf_attention_weights)].numpy().astype(dtype=np.int32)\n",
    "        classes           = [self.idToText(i) for i in classes]\n",
    "        x                 = np.arange(len(tf_attention_weights))\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.bar(x, tf_attention_weights)\n",
    "        plt.xticks(x, classes)\n",
    "        ax.set_ylim([0, 1])\n",
    "        plt.text(0.01, 0.95, self.voice.tokensToSentence(tf_language.numpy().tolist()), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Attention\", data=result, step=stepid)\n",
    "    \n",
    "    def plotClassAccuracy(self, gt_class, pred_class, pred_class_std, language, stepid):\n",
    "        labels     = [\"ysr\", \"rsr\", \"gsr\", \"bsr\", \"psr\", \"ylr\", \"rlr\", \"glr\", \"blr\", \"plr\", \"yss\", \"rss\", \"gss\", \"bss\", \"pss\", \"yls\", \"rls\", \"gls\", \"bls\", \"pls\"]\n",
    "        tf_gt_class = self.torch2tf(gt_class)\n",
    "        tf_pred_class = self.torch2tf(pred_class)\n",
    "        tf_language = self.torch2tf(language)\n",
    "\n",
    "        \n",
    "        \n",
    "        tf_gt_class   = tf_gt_class.numpy()\n",
    "        tf_pred_class = tf_pred_class.numpy()\n",
    "        x          = np.arange(len(tf_gt_class))\n",
    "        width      = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        #rects1 = ax.bar(x - width/2, gt_class, width, label='GT', color=\"forestgreen\")\n",
    "        #rects2 = ax.bar(x + width/2, pred_class, width, yerr=pred_class_std, label='Pred', color=\"midnightblue\")\n",
    "        ax.set_xticks(x)\n",
    "        # ax.set_xticklabels(labels)\n",
    "        plt.text(0.01, 0.95, self.voice.tokensToSentence(tf_language.numpy().tolist()), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Attention\", data=result, step=stepid)\n",
    "\n",
    "    def plotDeltaT(self, y_true, y_pred, stepid):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "\n",
    "        gt = tf_y_true.numpy()\n",
    "        pd = tf_y_pred.numpy()[:,0]\n",
    "        jdata = np.stack((gt,pd), axis=1)\n",
    "        svals = jdata[np.argsort(jdata[:,0]),:]\n",
    "        x     = np.arange(svals.shape[0])\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(x - width/2, svals[:,0], width, label='GT', color=\"forestgreen\")\n",
    "        rects2 = ax.bar(x + width/2, svals[:,1], width, label='Pred', color=\"midnightblue\")\n",
    "        ax.set_xticks(x)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"DeltaT\", data=result, step=stepid)\n",
    "\n",
    "    def plotWeights(self, gt_w, pred_w, stepid):\n",
    "        tf_gt_w = self.torch2tf(gt_w)\n",
    "        tf_pred_w = self.torch2tf(pred_w)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2,sharey=True,sharex=True)\n",
    "        # fig.set_size_inches(4, 10)\n",
    "\n",
    "        combined_weights = np.concatenate((tf_gt_w.numpy(), tf_pred_w.numpy()), axis=0).T\n",
    "\n",
    "        ax1.imshow(combined_weights[:,:7], cmap=\"RdBu\")\n",
    "        ax2.imshow(combined_weights[:,7:], cmap=\"RdBu\")\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Weights\", data=result, step=stepid)\n",
    "\n",
    "    def interpolateTrajectory(self, trj, target):\n",
    "        tf_trj = self.torch2tf(trj)\n",
    "        tf_target = self.torch2tf(target)\n",
    "\n",
    "        current_length = tf_trj.shape[0]\n",
    "        dimensions     = tf_trj.shape[1]\n",
    "        result         = np.zeros((tf_target, dimensions), dtype=np.float32)\n",
    "    \n",
    "        for i in range(dimensions):\n",
    "            result[:,i] = np.interp(np.linspace(0.0, 1.0, num=tf_target), np.linspace(0.0, 1.0, num=current_length), trj[:,i])\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def plotDMPTrajectory(self, y_true, y_pred, y_pred_std = None, phase= None, \\\n",
    "        dt= None, p_dt= None, stepid= None, name = \"Trajectory\", save = False, \\\n",
    "            name_plot = None, path=None, tol_neg = None, tol_pos=None, inpt = None, opt_gen_trj=None, window = 0):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "        tf_phase = self.torch2tf(phase)\n",
    "        tf_inpt = self.torch2tf(inpt.reshape(-1))\n",
    "        if p_dt is not None:\n",
    "            tf_dt = self.torch2tf(dt)\n",
    "            tf_p_dt = self.torch2tf(p_dt)\n",
    "        if opt_gen_trj is not None:\n",
    "            tf_opt_gen_trj = self.torch2tf(opt_gen_trj)\n",
    "            tf_opt_gen_trj = tf_opt_gen_trj.numpy()\n",
    "\n",
    "        tf_y_true      = tf_y_true.numpy()\n",
    "        tf_y_pred      = tf_y_pred.numpy()\n",
    "        tf_inpt        = tf_inpt.numpy()\n",
    "        if tf_phase is not None:\n",
    "            tf_phase       = tf_phase.numpy()\n",
    "\n",
    "        if p_dt is not None:\n",
    "            tf_dt          = tf_dt.numpy() * 350.0\n",
    "            tf_p_dt        = tf_p_dt.numpy()\n",
    "        trj_len      = tf_y_true.shape[0]\n",
    "        \n",
    "        #fig, ax = plt.subplots(3,3)\n",
    "        fig, ax = self.fig, self.ax\n",
    "        #fig.set_size_inches(9, 9)\n",
    "        neg_inpt = tf_y_true + tol_neg[None,:].cpu().numpy()\n",
    "        pos_inpt = tf_y_true + tol_pos[None,:].cpu().numpy()\n",
    "        for sp in range(len(tf_y_true[0])):\n",
    "            idx = sp // 3\n",
    "            idy = sp  % 3\n",
    "            ax[idx,idy].clear()\n",
    "\n",
    "            # GT Trajectory:\n",
    "            if tol_neg is not None:\n",
    "\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), neg_inpt[:,sp], alpha=0.75, color='orangered')\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), pos_inpt[:,sp], alpha=0.75, color='orangered')\n",
    "            ax[idx,idy].plot(range(trj_len), tf_y_true[:,sp],   alpha=1.0, color='forestgreen')            \n",
    "            ax[idx,idy].plot(range(tf_y_pred.shape[0]), tf_y_pred[:,sp], alpha=0.75, color='mediumslateblue')\n",
    "            if opt_gen_trj is not None:\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), tf_opt_gen_trj[:,sp], alpha=0.75, color='lightseagreen')\n",
    "                diff_vec = tf_opt_gen_trj - tf_y_pred\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), diff_vec[:,sp], alpha=0.75, color='pink')\n",
    "\n",
    "            #ax[idx,idy].errorbar(range(tf_y_pred.shape[0]), tf_y_pred[:,sp], xerr=None, yerr=None, alpha=0.25, fmt='none', color='mediumslateblue')\n",
    "            #ax[idx,idy].set_ylim([-0.1, 1.1])\n",
    "            if p_dt is not None:\n",
    "                ax[idx,idy].plot([tf_dt, tf_dt], [0.0,1.0], linestyle=\":\", color='forestgreen')\n",
    "\n",
    "        if inpt is not None:\n",
    "            ax[-1,-1].clear()\n",
    "            ax[-1,-1].plot(range(inpt.shape[-1]), tf_inpt,   alpha=1.0, color='forestgreen')     \n",
    "        \n",
    "        if tf_phase is not None:\n",
    "            ax[2,2].clear()\n",
    "            ax[2,2].plot(range(tf_y_pred.shape[0]), tf_phase, color='orange')\n",
    "        if p_dt is not None:\n",
    "            ax[2,2].plot([tf_dt, tf_dt], [0.0,1.0], linestyle=\":\", color='forestgreen')\n",
    "            ax[2,2].plot([tf_p_dt*350.0, tf_p_dt*350.0], [0.0,1.0], linestyle=\":\", color='mediumslateblue')\n",
    "            ax[2,2].set_ylim([-0.1, 1.1])\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        if save:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            plt.savefig(path + name_plot + '.png')\n",
    "        #fig.clear()\n",
    "        #plt.close()\n",
    "        if not save:\n",
    "            with self.__tboard_validation.as_default():\n",
    "                tf.summary.image(name, data=result, step=stepid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashids import Hashids\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tboard = TBoardGraphsTorch(logname='easy', data_path='/home/hendrik/Documents/master_project/LokalData/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.policies import *\n",
    "policy = ActorCriticPolicy(observation_space=toy_env.observation_space, action_space=toy_env.action_space, lr_schedule=lambda _: torch.finfo(torch.float32).max, net_arch = [dict(pi=[200, 200], vf=[200, 200])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = bc.BC(\n",
    "    observation_space=toy_env.observation_space,\n",
    "    action_space=toy_env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    policy=policy,\n",
    "    device='cpu'\n",
    ")\n",
    "#bc_trainer.train(n_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = TBoardGraphsTorch(logname='easy', data_path='/home/hendrik/Documents/master_project/LokalData/test/')\n",
    "for i in range(100):\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = bc_trainer.policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.30*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.45*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='imitation baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='imitation baseline', opt_gen_trj = None, window=None)\n",
    "    bc_trainer.train(n_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "vec_toy_env = make_vec_env(my_env, n_envs=1)\n",
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_TAKEN = 0\n",
    "rein_model = PPO(\"MlpPolicy\", vec_toy_env, verbose=0, learning_rate=1e-4)\n",
    "rein_model.policy = policy\n",
    "rein_model.policy.to(rein_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = TBoardGraphsTorch(logname='easy', data_path='/home/hendrik/Documents/master_project/LokalData/test/')\n",
    "for i in range(100):\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = rein_model.policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.30*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.45*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='imitation baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='imitation baseline', opt_gen_trj = None, window=None)\n",
    "    rein_model.learn(total_timesteps=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save('backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.load('backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLED_ENVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_TAKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_reward(policy, logname = 'ppo'):\n",
    "    tboard = TBoardGraphsTorch(logname=logname, data_path='/home/hendrik/Documents/master_project/LokalData/stableBaselines/')\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(f'num_envs: {SAMPLED_ENVS}')\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='ppo fine tuning baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='ppo fine tuning baseline', opt_gen_trj = None, window=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = bc_trainer.policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='imitation baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='imitation baseline', opt_gen_trj = None, window=None)\n",
    "    bc_trainer.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import HerReplayBuffer, DDPG, DQN, SAC, TD3\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "from stable_baselines3.common.envs import BitFlippingEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "model_class = DQN  # works also with SAC, DDPG and TD3\n",
    "N_BITS = 15\n",
    "\n",
    "env = BitFlippingEnv(n_bits=N_BITS, continuous=model_class in [DDPG, SAC, TD3], max_steps=N_BITS)\n",
    "\n",
    "# Available strategies (cf paper): future, final, episode\n",
    "goal_selection_strategy = 'future' # equivalent to GoalSelectionStrategy.FUTURE\n",
    "\n",
    "# If True the HER transitions will get sampled online\n",
    "online_sampling = True\n",
    "# Time limit for the episodes\n",
    "max_episode_length = N_BITS\n",
    "\n",
    "# Initialize the model\n",
    "model = model_class(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    # Parameters for HER\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=4,\n",
    "        goal_selection_strategy=goal_selection_strategy,\n",
    "        online_sampling=online_sampling,\n",
    "        max_episode_length=max_episode_length,\n",
    "    ),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(1000)\n",
    "\n",
    "model.save(\"./her_bit_env\")\n",
    "# Because it needs access to `env.compute_reward()`\n",
    "# HER must be loaded with the env\n",
    "model = model_class.load('./her_bit_env', env=env)\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('observation',\n",
       "              array([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1], dtype=int8)),\n",
       "             ('achieved_goal',\n",
       "              array([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1], dtype=int8)),\n",
       "             ('desired_goal',\n",
       "              array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6edd83f9b3fcb9454c0e509bb1e55f01736f244b1bbba81ee1367549d9ea0fd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mujoco')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
