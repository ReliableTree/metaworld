{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class asdasd(torch.utils.data.Dataset):\n",
    "    def __init__(self, asd):\n",
    "        print('init')\n",
    "        self.data = torch.arange(10)\n",
    "        print(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        print(len(self.data))\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class asd(torch.nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.lin = torch.nn.Linear(2,3)\n",
    "\n",
    "    def forward(self, inpt):\n",
    "        return self.lin(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import higher\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = asd()\n",
    "b = asd()\n",
    "c = [a,b]\n",
    "f = []\n",
    "for g in c:\n",
    "    f += list(g.parameters())\n",
    "d = torch.optim.AdamW(f, lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "len(train_data): 1\n"
     ]
    }
   ],
   "source": [
    "path = '/home/hendrik/Documents/master_project/LokalData/metaworld/pick-place/training_data/'\n",
    "device = 'cuda'\n",
    "batch_size = 2\n",
    "#train_data = TorchDatasetMW(path=path, device=device)\n",
    "train_data = asdasd(asd=1)\n",
    "train_indices = torch.randperm(int(len(train_data)))\n",
    "train_indices = train_indices[:int(len(train_indices)*1)]\n",
    "print(len(train_data))\n",
    "train_data = torch.utils.data.Subset(train_data, [0])\n",
    "print(f'len(train_data): {len(train_data)}')\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 4])\n",
      "torch.Size([10000, 5, 4])\n"
     ]
    }
   ],
   "source": [
    "path_train = '/home/hendrik/Documents/master_project/LokalData/metaworld/small/train/'\n",
    "path_validate = '/home/hendrik/Documents/master_project/LokalData/metaworld/small/val/'\n",
    "train_data = TorchDatasetMWToy(path=path_train, device='cpu')\n",
    "val_data = TorchDatasetMWToy(path=path_validate, device='cpu')\n",
    "print(train_data.data.shape)\n",
    "print(train_data.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global SAMPLED_ENVS\n",
    "global STEPS_TAKEN\n",
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_env():\n",
    "    def __init__(self, data=None):\n",
    "        if data is None:\n",
    "            data = train_data\n",
    "        #obs = step, data, action, current_env\n",
    "        self.observation_space = gym.spaces.box.Box(np.array([0, -2,-2,-2,-2, 0,0,0,0.,0]), np.array([6, 2,2,2,2, 1,1,1,1.,train_data.data.size(0)]), (10,), float)\n",
    "        #next state (4)\n",
    "        self.action_space = gym.spaces.box.Box(np.array([0,0,0,0]), np.array([1,1,1,1]), (4,), float)\n",
    "        self.metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}\n",
    "        self.steps = 0\n",
    "        self.current_env = -1\n",
    "        self.data = data\n",
    "        self.label = data.label\n",
    "        self.traj = None\n",
    "        self.num_envs = 1\n",
    "    def reset(self):\n",
    "        global SAMPLED_ENVS\n",
    "        global STEPS_TAKEN\n",
    "        STEPS_TAKEN += 1\n",
    "        SAMPLED_ENVS += 1\n",
    "        self.traj = None\n",
    "        self.current_env = (self.current_env + 1)%len(self.data)\n",
    "        self.steps = 0\n",
    "        last_action = torch.zeros(4, dtype=float, device=self.data.data.device)\n",
    "        step = torch.tensor(self.steps, device=self.data.data.device)\n",
    "        current_env = torch.tensor(self.current_env, device=self.data.data.device)\n",
    "        data = self.data.data[self.current_env, 0]\n",
    "        #label = self.label[self.current_env,0]\n",
    "        state = torch.cat((step.view(1), data, last_action, current_env.view(1)), dim=0).numpy()\n",
    "        #print(f'reset: {state.shape}')\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        global STEPS_TAKEN\n",
    "        STEPS_TAKEN += 1\n",
    "        if type(action) is np.ndarray:\n",
    "            action = torch.tensor(action, device=self.data.data.device)\n",
    "        if self.traj is None:\n",
    "            self.traj = action.reshape(1,-1)\n",
    "        else:\n",
    "            self.traj = torch.cat((self.traj, action.reshape(1,-1)), dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        self.steps += 1\n",
    "        step = torch.tensor(self.steps, device=self.data.data.device)\n",
    "        current_env = torch.tensor(self.current_env, device=self.data.data.device)\n",
    "\n",
    "        #label = self.label[self.current_env, self.current_step]\n",
    "        data = self.data.data[self.current_env, 0]\n",
    "\n",
    "        state = torch.cat((step.view(1), data, action.reshape(-1), current_env.view(1)), dim=0).numpy()\n",
    "        #print(f'step: {state.shape}')\n",
    "\n",
    "        if self.steps >= self.label.size(1):\n",
    "            tol_neg = -0.55*torch.ones([self.traj.size(-1)])\n",
    "            tol_pos = 0.7*torch.ones([self.traj.size(-1)])\n",
    "            reward = int(check_outpt(self.label[self.current_env].unsqueeze(0), self.traj.unsqueeze(0), tol_neg=tol_neg, tol_pos=tol_pos))\n",
    "            return (state, reward, True, {})\n",
    "        else:\n",
    "            return (state, 0., False, {})\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def render(self, mode):\n",
    "        pass\n",
    "\n",
    "class toy_exper_model(OnPolicyAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            policy: Union[str, Type[ActorCriticPolicy]] = 'MlpPolicy',\n",
    "            env: Union[GymEnv, str] = None,\n",
    "            learning_rate: Union[float, Schedule] = 3e-4,\n",
    "            n_steps: int = 2048,\n",
    "            batch_size: int = 64,\n",
    "            n_epochs: int = 10,\n",
    "            gamma: float = 0.99,\n",
    "            gae_lambda: float = 0.95,\n",
    "            clip_range: Union[float, Schedule] = 0.2,\n",
    "            clip_range_vf: Union[None, float, Schedule] = None,\n",
    "            normalize_advantage: bool = True,\n",
    "            ent_coef: float = 0.0,\n",
    "            vf_coef: float = 0.5,\n",
    "            max_grad_norm: float = 0.5,\n",
    "            use_sde: bool = False,\n",
    "            sde_sample_freq: int = -1,\n",
    "            target_kl: Optional[float] = None,\n",
    "            tensorboard_log: Optional[str] = None,\n",
    "            create_eval_env: bool = False,\n",
    "            policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            verbose: int = 0,\n",
    "            seed: Optional[int] = None,\n",
    "            device: Union[th.device, str] = \"auto\",\n",
    "            _init_setup_model: bool = True,\n",
    "            train_data = None\n",
    "        ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            create_eval_env=create_eval_env,\n",
    "            seed=seed,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "        self.data = train_data.data\n",
    "        self.label = train_data.label\n",
    "        #obs = step, data, action, current_env\n",
    "        self.observation_space = gym.spaces.box.Box(np.array([0, -2,-2,-2,-2, 0,0,0,0.,0]), np.array([6, 2,2,2,2, 1,1,1,1.,train_data.data.size(0)]), (10,), float)\n",
    "        #next state (4)\n",
    "        self.action_space = gym.spaces.box.Box(np.array([0,0,0,0]), np.array([1,1,1,1]), (4,), float)\n",
    "\n",
    "    def predict(self, obs, state=None, episode_start=None, deterministic=False):\n",
    "        step = int(obs.reshape(-1)[0])\n",
    "        env = int(obs.reshape(-1)[-1])\n",
    "        #print(f'expert: {self.label[env, step].reshape(1, -1).shape}')\n",
    "        return self.label[env, step].reshape(1, -1), self.label[env, step].reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_env = my_env(data=train_data)\n",
    "val_env = my_env(data=val_data)\n",
    "my_expert = toy_exper_model(train_data=train_data, env=toy_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions():\n",
    "    expert = my_expert\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(toy_env)]),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=10000),\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling expert transitions.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000016?line=0'>1</a>\u001b[0m transitions \u001b[39m=\u001b[39m sample_expert_transitions()\n",
      "\u001b[1;32m/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb Cell 16'\u001b[0m in \u001b[0;36msample_expert_transitions\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000015?line=1'>2</a>\u001b[0m expert \u001b[39m=\u001b[39m my_expert\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000015?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSampling expert transitions.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000015?line=4'>5</a>\u001b[0m rollouts \u001b[39m=\u001b[39m rollout\u001b[39m.\u001b[39;49mrollout(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000015?line=5'>6</a>\u001b[0m     expert,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000015?line=6'>7</a>\u001b[0m     DummyVecEnv([\u001b[39mlambda\u001b[39;49;00m: RolloutInfoWrapper(toy_env)]),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000015?line=7'>8</a>\u001b[0m     rollout\u001b[39m.\u001b[39;49mmake_sample_until(min_timesteps\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, min_episodes\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000015?line=8'>9</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000015?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m rollout\u001b[39m.\u001b[39mflatten_trajectories(rollouts)\n",
      "File \u001b[0;32m~/Documents/master_project/Code/imitation/src/imitation/data/rollout.py:588\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(policy, venv, sample_until, unwrap, exclude_infos, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=551'>552</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrollout\u001b[39m(\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=552'>553</a>\u001b[0m     policy: AnyPolicy,\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=553'>554</a>\u001b[0m     venv: VecEnv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=559'>560</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=560'>561</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Sequence[types\u001b[39m.\u001b[39mTrajectoryWithRew]:\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=561'>562</a>\u001b[0m     \u001b[39m\"\"\"Generate policy rollouts.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=562'>563</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=563'>564</a>\u001b[0m \u001b[39m    The `.infos` field of each Trajectory is set to `None` to save space.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=585'>586</a>\u001b[0m \u001b[39m        should truncate if required.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=586'>587</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=587'>588</a>\u001b[0m     trajs \u001b[39m=\u001b[39m generate_trajectories(policy, venv, sample_until, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=588'>589</a>\u001b[0m     \u001b[39mif\u001b[39;00m unwrap:\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=589'>590</a>\u001b[0m         trajs \u001b[39m=\u001b[39m [unwrap_traj(traj) \u001b[39mfor\u001b[39;00m traj \u001b[39min\u001b[39;00m trajs]\n",
      "File \u001b[0;32m~/Documents/master_project/Code/imitation/src/imitation/data/rollout.py:344\u001b[0m, in \u001b[0;36mgenerate_trajectories\u001b[0;34m(policy, venv, sample_until, deterministic_policy, rng)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=341'>342</a>\u001b[0m \u001b[39m# accumulator for incomplete trajectories\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=342'>343</a>\u001b[0m trajectories_accum \u001b[39m=\u001b[39m TrajectoryAccumulator()\n\u001b[0;32m--> <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=343'>344</a>\u001b[0m obs \u001b[39m=\u001b[39m venv\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=344'>345</a>\u001b[0m \u001b[39mfor\u001b[39;00m env_idx, ob \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(obs):\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=345'>346</a>\u001b[0m     \u001b[39m# Seed with first obs only. Inside loop, we'll only add second obs from\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=346'>347</a>\u001b[0m     \u001b[39m# each (s,a,r,s') tuple, under the same \"obs\" key again. That way we still\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=347'>348</a>\u001b[0m     \u001b[39m# get all observations, but they're not duplicated into \"next obs\" and\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=348'>349</a>\u001b[0m     \u001b[39m# \"previous obs\" (this matters for, e.g., Atari, where observations are\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=349'>350</a>\u001b[0m     \u001b[39m# really big).\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/rollout.py?line=350'>351</a>\u001b[0m     trajectories_accum\u001b[39m.\u001b[39madd_step(\u001b[39mdict\u001b[39m(obs\u001b[39m=\u001b[39mob), env_idx)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:61\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/hendrik/anaconda3/envs/mujoco/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=58'>59</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvObs:\n\u001b[1;32m     <a href='file:///home/hendrik/anaconda3/envs/mujoco/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=59'>60</a>\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> <a href='file:///home/hendrik/anaconda3/envs/mujoco/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=60'>61</a>\u001b[0m         obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m     <a href='file:///home/hendrik/anaconda3/envs/mujoco/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=61'>62</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     <a href='file:///home/hendrik/anaconda3/envs/mujoco/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py?line=62'>63</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[0;32m~/Documents/master_project/Code/imitation/src/imitation/data/wrappers.py:179\u001b[0m, in \u001b[0;36mRolloutInfoWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/wrappers.py?line=177'>178</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/wrappers.py?line=178'>179</a>\u001b[0m     new_obs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/wrappers.py?line=179'>180</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs \u001b[39m=\u001b[39m [new_obs]\n\u001b[1;32m    <a href='file:///home/hendrik/Documents/master_project/Code/imitation/src/imitation/data/wrappers.py?line=180'>181</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rews \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.8/site-packages/gym/core.py:292\u001b[0m, in \u001b[0;36mWrapper.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hendrik/anaconda3/envs/mujoco/lib/python3.8/site-packages/gym/core.py?line=290'>291</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///home/hendrik/anaconda3/envs/mujoco/lib/python3.8/site-packages/gym/core.py?line=291'>292</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mreset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb Cell 14'\u001b[0m in \u001b[0;36mmy_env.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000013?line=26'>27</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_env, \u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000013?line=27'>28</a>\u001b[0m \u001b[39m#label = self.label[self.current_env,0]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000013?line=28'>29</a>\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((step\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m), data, last_action, current_env\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m)), dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000013?line=29'>30</a>\u001b[0m \u001b[39m#print(f'reset: {state.shape}')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/stable_baseline.ipynb#ch0000013?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m state\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "transitions = sample_expert_transitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Simon Stepputtis <sstepput@asu.edu>, Interactive Robotics Lab, Arizona State University\n",
    "\n",
    "from pickle import NONE\n",
    "from urllib.parse import non_hierarchical\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from hashids import Hashids\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class TBoardGraphsTorch():\n",
    "    def __init__(self, logname= None, data_path = None):\n",
    "        if logname is not None:\n",
    "            self.__hashids           = Hashids()\n",
    "            #self.logdir              = \"Data/TBoardLog/\" + logname + \"/\"\n",
    "            self.logdir              = os.path.join(data_path, \"gboard/\" + logname + \"/\")\n",
    "            print(f'log dir: {self.logdir + \"train/\"}')\n",
    "            self.__tboard_train      = tf.summary.create_file_writer(self.logdir + \"train/\")\n",
    "            self.__tboard_validation = tf.summary.create_file_writer(self.logdir + \"validate/\")\n",
    "            #self.voice               = Voice(path=data_path)\n",
    "        self.fig, self.ax = plt.subplots(3,3)\n",
    "\n",
    "    def startDebugger(self):\n",
    "        tf.summary.trace_on(graph=True, profiler=True)\n",
    "    \n",
    "    def stopDebugger(self):\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=self.logdir)\n",
    "\n",
    "    def finishFigure(self, fig):\n",
    "        fig.canvas.draw()\n",
    "        data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        return data\n",
    "    \n",
    "    def addTrainScalar(self, name, value, stepid):\n",
    "        with self.__tboard_train.as_default():\n",
    "            tfvalue = self.torch2tf(value)\n",
    "            tf.summary.scalar(name, tfvalue, step=stepid)\n",
    "\n",
    "    def addValidationScalar(self, name, value, stepid):\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tfvalue = self.torch2tf(value)\n",
    "            tf.summary.scalar(name, tfvalue, step=stepid)\n",
    "\n",
    "    def torch2tf(self, inpt):\n",
    "        if inpt is not None:\n",
    "            return tf.convert_to_tensor(inpt.detach().cpu().numpy())\n",
    "        else:\n",
    "            return inpt\n",
    "\n",
    "    def plotTrajectory(self, y_true, y_pred, dt_true, dt_pred, stepid):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "        tf_dt_true = self.torch2tf(dt_true)\n",
    "        tf_dt_pred = self.torch2tf(dt_pred)\n",
    "\n",
    "        fig, ax = plt.subplots(3,3)\n",
    "        fig.set_size_inches(9, 9)\n",
    "\n",
    "        tf_dt_true = 1.0/tf_dt_true.numpy()\n",
    "        tf_dt_pred = 1.0/tf_dt_pred.numpy()[0]\n",
    "\n",
    "        max_trj_len = tf_y_true.shape[0]\n",
    "        for sp in range(7):\n",
    "            idx = sp // 3\n",
    "            idy = sp  % 3\n",
    "            ax[idx,idy].clear()\n",
    "            ax[idx,idy].plot(range(max_trj_len), tf_y_pred[:,sp], alpha=0.5, color='midnightblue')\n",
    "            ax[idx,idy].plot(range(max_trj_len), tf_y_true[:,sp], alpha=0.5, color='forestgreen')\n",
    "            # ax[idx,idy].plot([dt_pred, dt_pred], [-0.1, 1.1], alpha=0.5, linestyle=\":\", color=\"midnightblue\")\n",
    "            # ax[idx,idy].plot([dt_true, dt_true], [-0.1, 1.1], alpha=0.5, linestyle=\":\", color=\"forestgreen\")\n",
    "            # ax[idx,idy].set_ylim([-0.1, 1.1])\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Trajectory\", data=result, step=stepid)\n",
    "\n",
    "    def idToText(self, id):\n",
    "        names = [\"\", \"ysr\", \"rsr\", \"gsr\", \"bsr\", \"psr\", \"ylr\", \"rlr\", \"glr\", \"blr\", \"plr\", \"yss\", \"rss\", \"gss\", \"bss\", \"pss\", \"yls\", \"rls\", \"gls\", \"bls\", \"pls\"]\n",
    "        return names[id]\n",
    "\n",
    "    def plotImageRegions(self, image, image_dict, stepid):\n",
    "        # Visualization of the results of a detection.\n",
    "        num_detected = len([v for v in image_dict[\"detection_scores\"][0] if v > 0.5]) \n",
    "        image_np     = image.numpy()       \n",
    "        for i in range(num_detected):\n",
    "            ymin, xmin, ymax, xmax = image_dict['detection_boxes'][0][i,:]\n",
    "            pt1 = (int(xmin*image_np.shape[1]), int(ymin*image_np.shape[0]))\n",
    "            pt2 = (int(xmax*image_np.shape[1]), int(ymax*image_np.shape[0]))\n",
    "            image_np = cv2.rectangle(image_np, pt1, pt2, (255, 0, 0), 2)\n",
    "            image_np = cv2.putText(image_np, self.idToText(image_dict['detection_classes'][0][i]) + \" {:.1f}%\".format(image_dict[\"detection_scores\"][0][i] * 100), pt1, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(image_np)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Image\", data=result, step=stepid)\n",
    "\n",
    "    def plotAttention(self, attention_weights, image_dict, language, stepid):\n",
    "        tf_attention_weights = self.torch2tf(attention_weights)\n",
    "        tf_language = self.torch2tf(language)\n",
    "\n",
    "        tf_attention_weights = tf_attention_weights.numpy()\n",
    "        classes           = image_dict[\"detection_classes\"][0][:len(tf_attention_weights)].numpy().astype(dtype=np.int32)\n",
    "        classes           = [self.idToText(i) for i in classes]\n",
    "        x                 = np.arange(len(tf_attention_weights))\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.bar(x, tf_attention_weights)\n",
    "        plt.xticks(x, classes)\n",
    "        ax.set_ylim([0, 1])\n",
    "        plt.text(0.01, 0.95, self.voice.tokensToSentence(tf_language.numpy().tolist()), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Attention\", data=result, step=stepid)\n",
    "    \n",
    "    def plotClassAccuracy(self, gt_class, pred_class, pred_class_std, language, stepid):\n",
    "        labels     = [\"ysr\", \"rsr\", \"gsr\", \"bsr\", \"psr\", \"ylr\", \"rlr\", \"glr\", \"blr\", \"plr\", \"yss\", \"rss\", \"gss\", \"bss\", \"pss\", \"yls\", \"rls\", \"gls\", \"bls\", \"pls\"]\n",
    "        tf_gt_class = self.torch2tf(gt_class)\n",
    "        tf_pred_class = self.torch2tf(pred_class)\n",
    "        tf_language = self.torch2tf(language)\n",
    "\n",
    "        \n",
    "        \n",
    "        tf_gt_class   = tf_gt_class.numpy()\n",
    "        tf_pred_class = tf_pred_class.numpy()\n",
    "        x          = np.arange(len(tf_gt_class))\n",
    "        width      = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        #rects1 = ax.bar(x - width/2, gt_class, width, label='GT', color=\"forestgreen\")\n",
    "        #rects2 = ax.bar(x + width/2, pred_class, width, yerr=pred_class_std, label='Pred', color=\"midnightblue\")\n",
    "        ax.set_xticks(x)\n",
    "        # ax.set_xticklabels(labels)\n",
    "        plt.text(0.01, 0.95, self.voice.tokensToSentence(tf_language.numpy().tolist()), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Attention\", data=result, step=stepid)\n",
    "\n",
    "    def plotDeltaT(self, y_true, y_pred, stepid):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "\n",
    "        gt = tf_y_true.numpy()\n",
    "        pd = tf_y_pred.numpy()[:,0]\n",
    "        jdata = np.stack((gt,pd), axis=1)\n",
    "        svals = jdata[np.argsort(jdata[:,0]),:]\n",
    "        x     = np.arange(svals.shape[0])\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(x - width/2, svals[:,0], width, label='GT', color=\"forestgreen\")\n",
    "        rects2 = ax.bar(x + width/2, svals[:,1], width, label='Pred', color=\"midnightblue\")\n",
    "        ax.set_xticks(x)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"DeltaT\", data=result, step=stepid)\n",
    "\n",
    "    def plotWeights(self, gt_w, pred_w, stepid):\n",
    "        tf_gt_w = self.torch2tf(gt_w)\n",
    "        tf_pred_w = self.torch2tf(pred_w)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2,sharey=True,sharex=True)\n",
    "        # fig.set_size_inches(4, 10)\n",
    "\n",
    "        combined_weights = np.concatenate((tf_gt_w.numpy(), tf_pred_w.numpy()), axis=0).T\n",
    "\n",
    "        ax1.imshow(combined_weights[:,:7], cmap=\"RdBu\")\n",
    "        ax2.imshow(combined_weights[:,7:], cmap=\"RdBu\")\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Weights\", data=result, step=stepid)\n",
    "\n",
    "    def interpolateTrajectory(self, trj, target):\n",
    "        tf_trj = self.torch2tf(trj)\n",
    "        tf_target = self.torch2tf(target)\n",
    "\n",
    "        current_length = tf_trj.shape[0]\n",
    "        dimensions     = tf_trj.shape[1]\n",
    "        result         = np.zeros((tf_target, dimensions), dtype=np.float32)\n",
    "    \n",
    "        for i in range(dimensions):\n",
    "            result[:,i] = np.interp(np.linspace(0.0, 1.0, num=tf_target), np.linspace(0.0, 1.0, num=current_length), trj[:,i])\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def plotDMPTrajectory(self, y_true, y_pred, y_pred_std = None, phase= None, \\\n",
    "        dt= None, p_dt= None, stepid= None, name = \"Trajectory\", save = False, \\\n",
    "            name_plot = None, path=None, tol_neg = None, tol_pos=None, inpt = None, opt_gen_trj=None, window = 0):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "        tf_phase = self.torch2tf(phase)\n",
    "        tf_inpt = self.torch2tf(inpt)\n",
    "        if p_dt is not None:\n",
    "            tf_dt = self.torch2tf(dt)\n",
    "            tf_p_dt = self.torch2tf(p_dt)\n",
    "        if opt_gen_trj is not None:\n",
    "            tf_opt_gen_trj = self.torch2tf(opt_gen_trj)\n",
    "            tf_opt_gen_trj = tf_opt_gen_trj.numpy()\n",
    "\n",
    "        tf_y_true      = tf_y_true.numpy()\n",
    "        tf_y_pred      = tf_y_pred.numpy()\n",
    "        tf_inpt        = tf_inpt.numpy()\n",
    "        if tf_phase is not None:\n",
    "            tf_phase       = tf_phase.numpy()\n",
    "\n",
    "        if p_dt is not None:\n",
    "            tf_dt          = tf_dt.numpy() * 350.0\n",
    "            tf_p_dt        = tf_p_dt.numpy()\n",
    "        trj_len      = tf_y_true.shape[0]\n",
    "        \n",
    "        #fig, ax = plt.subplots(3,3)\n",
    "        fig, ax = self.fig, self.ax\n",
    "        #fig.set_size_inches(9, 9)\n",
    "        neg_inpt = tf_y_true + tol_neg[None,:].cpu().numpy()\n",
    "        pos_inpt = tf_y_true + tol_pos[None,:].cpu().numpy()\n",
    "        for sp in range(len(tf_y_true[0])):\n",
    "            idx = sp // 3\n",
    "            idy = sp  % 3\n",
    "            ax[idx,idy].clear()\n",
    "\n",
    "            # GT Trajectory:\n",
    "            if tol_neg is not None:\n",
    "\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), neg_inpt[:,sp], alpha=0.75, color='orangered')\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), pos_inpt[:,sp], alpha=0.75, color='orangered')\n",
    "            ax[idx,idy].plot(range(trj_len), tf_y_true[:,sp],   alpha=1.0, color='forestgreen')            \n",
    "            ax[idx,idy].plot(range(tf_y_pred.shape[0]), tf_y_pred[:,sp], alpha=0.75, color='mediumslateblue')\n",
    "            if opt_gen_trj is not None:\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), tf_opt_gen_trj[:,sp], alpha=0.75, color='lightseagreen')\n",
    "                diff_vec = tf_opt_gen_trj - tf_y_pred\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), diff_vec[:,sp], alpha=0.75, color='pink')\n",
    "\n",
    "            #ax[idx,idy].errorbar(range(tf_y_pred.shape[0]), tf_y_pred[:,sp], xerr=None, yerr=None, alpha=0.25, fmt='none', color='mediumslateblue')\n",
    "            #ax[idx,idy].set_ylim([-0.1, 1.1])\n",
    "            if p_dt is not None:\n",
    "                ax[idx,idy].plot([tf_dt, tf_dt], [0.0,1.0], linestyle=\":\", color='forestgreen')\n",
    "\n",
    "        if inpt is not None:\n",
    "            ax[-1,-1].clear()\n",
    "            ax[-1,-1].plot(range(inpt.shape[-1]), tf_inpt,   alpha=1.0, color='forestgreen')     \n",
    "        \n",
    "        if tf_phase is not None:\n",
    "            ax[2,2].clear()\n",
    "            ax[2,2].plot(range(tf_y_pred.shape[0]), tf_phase, color='orange')\n",
    "        if p_dt is not None:\n",
    "            ax[2,2].plot([tf_dt, tf_dt], [0.0,1.0], linestyle=\":\", color='forestgreen')\n",
    "            ax[2,2].plot([tf_p_dt*350.0, tf_p_dt*350.0], [0.0,1.0], linestyle=\":\", color='mediumslateblue')\n",
    "            ax[2,2].set_ylim([-0.1, 1.1])\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        if save:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            plt.savefig(path + name_plot + '.png')\n",
    "        #fig.clear()\n",
    "        #plt.close()\n",
    "        if not save:\n",
    "            with self.__tboard_validation.as_default():\n",
    "                tf.summary.image(name, data=result, step=stepid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashids import Hashids\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tboard = TBoardGraphsTorch(logname='asd', data_path='/home/hendrik/Documents/master_project/LokalData/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.policies import *\n",
    "policy = ActorCriticPolicy(observation_space=toy_env.observation_space, action_space=toy_env.action_space, lr_schedule=lambda _: torch.finfo(torch.float32).max, net_arch = [dict(pi=[200, 200], vf=[200, 200])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = bc.BC(\n",
    "    observation_space=toy_env.observation_space,\n",
    "    action_space=toy_env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    policy=policy,\n",
    "    device='cpu'\n",
    ")\n",
    "#bc_trainer.train(n_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = TBoardGraphsTorch(logname='asd', data_path='/home/hendrik/Documents/master_project/LokalData/test/')\n",
    "for i in range(100):\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = bc_trainer.policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='imitation baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='imitation baseline', opt_gen_trj = None, window=None)\n",
    "    bc_trainer.train(n_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "vec_toy_env = make_vec_env(my_env, n_envs=1)\n",
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save('backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.load('backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0\n",
    "rein_model = PPO(\"MlpPolicy\", vec_toy_env, verbose=0, learning_rate=1e-4)\n",
    "rein_model.policy = policy\n",
    "rein_model.policy.to(rein_model.device)\n",
    "\n",
    "for i in range(100):\n",
    "    rein_model.learn(total_timesteps=1000)\n",
    "    print_reward(rein_model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLED_ENVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_TAKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_reward(policy, logname = 'ppo'):\n",
    "    tboard = TBoardGraphsTorch(logname=logname, data_path='/home/hendrik/Documents/master_project/LokalData/stableBaselines/')\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(f'num_envs: {SAMPLED_ENVS}')\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='ppo fine tuning baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='ppo fine tuning baseline', opt_gen_trj = None, window=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = bc_trainer.policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='imitation baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='imitation baseline', opt_gen_trj = None, window=None)\n",
    "    bc_trainer.train(n_epochs=5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6edd83f9b3fcb9454c0e509bb1e55f01736f244b1bbba81ee1367549d9ea0fd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mujoco')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
