{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "learner.use_alt_policy = False\n",
    "learner.alternative_policy = vec_expert\n",
    "tboard = TBoardGraphs(logname='TQC without expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(5000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n",
    "    print(env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "2023-02-07 10:11:58.275108: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "import argparse\n",
    "from sb3_contrib import TQC\n",
    "\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "def run_experiment(device):\n",
    "\n",
    "    env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "    val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "    transitions, _ = sample_expert_transitions(vec_expert.predict, env, 20)\n",
    "    env.envs[0].reset_count = 0\n",
    "\n",
    "    reward_net = BasicRewardNet(\n",
    "        env.observation_space, env.action_space, normalize_input_layer=RunningNorm)\n",
    "\n",
    "    policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "    learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05, device=device)\n",
    "    \n",
    "    gail_trainer = gail.GAIL(\n",
    "        demonstrations=transitions,\n",
    "        demo_batch_size=1024,\n",
    "        gen_replay_buffer_capacity=2048,\n",
    "        n_disc_updates_per_round=4,\n",
    "        venv=env,\n",
    "        gen_algo=learner,\n",
    "        reward_net=reward_net\n",
    "\n",
    "    )\n",
    "    tboard = TBoardGraphs(logname='GAIL + TQC pickplace 100', data_path='/data/bing/hendrik/gboard/')\n",
    "\n",
    "    for i in range(10000):\n",
    "        gail_trainer.train(100)\n",
    "        print(f'asd: {env.envs[0].reset_count}')\n",
    "        print(f'ddd: {val_env.envs[0].reset_count}')\n",
    "        print(f'fff: {env.envs[0].current_step}')\n",
    "        '''succ, rew = asd(env=val_env, learner=learner)\n",
    "        print('____________________________________________')\n",
    "        print(env.envs[0].reset_count)\n",
    "        print(val_env.envs[0].reset_count)\n",
    "        tboard.addTrainScalar('Reward', value=th.tensor(rew.mean()), stepid=env.envs[0].reset_count)\n",
    "        tboard.addTrainScalar('Success Rate', value=th.tensor(succ.mean()), stepid=env.envs[0].reset_count)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/hendrik/Documents/master_project/Code/MetaWorld/metaworld/policies/policy.py:41: UserWarning: Constant(s) may be too high. Environments clip response to [-1, 1]\n",
      "  warnings.warn('Constant(s) may be too high. Environments clip response to [-1, 1]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asddddddd\n",
      "asddddddd\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n",
      "reset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "2023-02-07 10:12:01.619058: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset 1\n",
      "Using cpu device\n",
      "reset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round: 100%|██████████| 100/100 [00:07<00:00, 14.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd: 0\n",
      "ddd: 0\n",
      "fff: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round:  99%|█████████▉| 99/100 [00:42<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round: 100%|██████████| 100/100 [00:42<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asd: 0\n",
      "ddd: 0\n",
      "fff: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round:  36%|███▌      | 36/100 [00:16<00:29,  2.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_experiment(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [1], line 87\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m     84\u001b[0m tboard \u001b[39m=\u001b[39m TBoardGraphs(logname\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGAIL + TQC pickplace 100\u001b[39m\u001b[39m'\u001b[39m, data_path\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/data/bing/hendrik/gboard/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m):\n\u001b[0;32m---> 87\u001b[0m     gail_trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m100\u001b[39;49m)\n\u001b[1;32m     88\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39masd: \u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39menvs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreset_count\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mddd: \u001b[39m\u001b[39m{\u001b[39;00mval_env\u001b[39m.\u001b[39menvs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mreset_count\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/imitation/algorithms/adversarial/common.py:423\u001b[0m, in \u001b[0;36mAdversarialTrainer.train\u001b[0;34m(self, total_timesteps, callback)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39massert\u001b[39;00m n_rounds \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, (\n\u001b[1;32m    418\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mNo updates (need at least \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen_train_timesteps\u001b[39m}\u001b[39;00m\u001b[39m timesteps, have only \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtotal_timesteps=\u001b[39m\u001b[39m{\u001b[39;00mtotal_timesteps\u001b[39m}\u001b[39;00m\u001b[39m)!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    421\u001b[0m )\n\u001b[1;32m    422\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n_rounds), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mround\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_gen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen_train_timesteps)\n\u001b[1;32m    424\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_disc_updates_per_round):\n\u001b[1;32m    425\u001b[0m         \u001b[39mwith\u001b[39;00m networks\u001b[39m.\u001b[39mtraining(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_train):\n\u001b[1;32m    426\u001b[0m             \u001b[39m# switch to training mode (affects dropout, normalization)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/imitation/algorithms/adversarial/common.py:383\u001b[0m, in \u001b[0;36mAdversarialTrainer.train_gen\u001b[0;34m(self, total_timesteps, learn_kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m     learn_kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    382\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39maccumulate_means(\u001b[39m\"\u001b[39m\u001b[39mgen\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 383\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen_algo\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    384\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    385\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    386\u001b[0m         callback\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen_callback,\n\u001b[1;32m    387\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mlearn_kwargs,\n\u001b[1;32m    388\u001b[0m     )\n\u001b[1;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_global_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    391\u001b[0m gen_trajs, ep_lens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvenv_buffering\u001b[39m.\u001b[39mpop_trajectories()\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/sb3_contrib/tqc/tqc.py:305\u001b[0m, in \u001b[0;36mTQC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m: TQCSelf,\n\u001b[1;32m    293\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    303\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TQCSelf:\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    306\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    307\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    308\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    309\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    310\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    311\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    312\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    313\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    314\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    315\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    316\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:381\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    380\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 381\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[1;32m    382\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_gradient_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m gradient_steps\n\u001b[1;32m    385\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/sb3_contrib/tqc/tqc.py:273\u001b[0m, in \u001b[0;36mTQC.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m# Optimize the actor\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 273\u001b[0m actor_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    276\u001b[0m \u001b[39m# Update target networks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_experiment('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "transitions, _ = sample_expert_transitions(vec_expert.predict, env, 20)\n",
    "env.envs[0].reset_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "reward_net = BasicRewardNet(\n",
    "    env.observation_space, env.action_space, normalize_input_layer=RunningNorm)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05, device=device)\n",
    "\n",
    "gail_trainer = gail.GAIL(\n",
    "    demonstrations=transitions,\n",
    "    demo_batch_size=1024,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=env,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions.obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_policy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class asdasd(torch.utils.data.Dataset):\n",
    "    def __init__(self, asd):\n",
    "        print('init')\n",
    "        self.data = torch.arange(10)\n",
    "        print(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        print(len(self.data))\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/hendrik/Documents/master_project/LokalData/metaworld/pick-place/training_data/'\n",
    "device = 'cuda'\n",
    "batch_size = 2\n",
    "#train_data = TorchDatasetMW(path=path, device=device)\n",
    "train_data = asdasd(asd=1)\n",
    "train_indices = torch.randperm(int(len(train_data)))\n",
    "train_indices = train_indices[:int(len(train_indices)*1)]\n",
    "print(len(train_data))\n",
    "train_data = torch.utils.data.Subset(train_data, [0])\n",
    "print(f'len(train_data): {len(train_data)}')\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '/home/hendrik/Documents/master_project/LokalData/metaworld/small/train/'\n",
    "path_validate = '/home/hendrik/Documents/master_project/LokalData/metaworld/small/val/'\n",
    "train_data = TorchDatasetMWToy(path=path_train, device='cpu')\n",
    "val_data = TorchDatasetMWToy(path=path_validate, device='cpu')\n",
    "print(train_data.data.shape)\n",
    "print(train_data.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global SAMPLED_ENVS\n",
    "global STEPS_TAKEN\n",
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_env():\n",
    "    def __init__(self):\n",
    "        #obs = step, data, action, current_env\n",
    "        self.observation_space = gym.spaces.box.Box(np.array([0, -2,-2,-2,-2, 0,0,0,0.,0]), np.array([6, 2,2,2,2, 1,1,1,1.,train_data.data.size(0)]), (10,), float)\n",
    "        #next state (4)\n",
    "        self.action_space = gym.spaces.box.Box(np.array([0,0,0,0]), np.array([1,1,1,1]), (4,), float)\n",
    "        self.metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}\n",
    "        self.steps = 0\n",
    "        self.current_env = -1\n",
    "        self.data = train_data.data\n",
    "        self.label = train_data.label\n",
    "        self.traj = None\n",
    "        self.num_envs = 1\n",
    "    def reset(self):\n",
    "        global SAMPLED_ENVS\n",
    "        global STEPS_TAKEN\n",
    "        STEPS_TAKEN += 1\n",
    "        SAMPLED_ENVS += 1\n",
    "        self.traj = None\n",
    "        self.current_env = (self.current_env + 1)%len(self.data)\n",
    "        self.steps = 0\n",
    "        last_action = torch.zeros(4, dtype=float, device=self.data.device)\n",
    "        step = torch.tensor(self.steps, device=self.data.device)\n",
    "        current_env = torch.tensor(self.current_env, device=self.data.device)\n",
    "        data = self.data[self.current_env, 0]\n",
    "        #label = self.label[self.current_env,0]\n",
    "        state = torch.cat((step.view(1), data, last_action, current_env.view(1)), dim=0).numpy()\n",
    "        #print(f'reset: {state.shape}')\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        global STEPS_TAKEN\n",
    "        STEPS_TAKEN += 1\n",
    "        if type(action) is np.ndarray:\n",
    "            action = torch.tensor(action, device=self.data.device)\n",
    "        if self.traj is None:\n",
    "            self.traj = action.reshape(1,-1)\n",
    "        else:\n",
    "            self.traj = torch.cat((self.traj, action.reshape(1,-1)), dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        self.steps += 1\n",
    "        step = torch.tensor(self.steps, device=self.data.device)\n",
    "        current_env = torch.tensor(self.current_env, device=self.data.device)\n",
    "\n",
    "        #label = self.label[self.current_env, self.current_step]\n",
    "        data = self.data[self.current_env, 0]\n",
    "\n",
    "        state = torch.cat((step.view(1), data, action.reshape(-1), current_env.view(1)), dim=0).numpy()\n",
    "        #print(f'step: {state.shape}')\n",
    "\n",
    "        if self.steps >= self.label.size(1):\n",
    "            tol_neg = -0.55*torch.ones([self.traj.size(-1)])\n",
    "            tol_pos = 0.7*torch.ones([self.traj.size(-1)])\n",
    "            reward = int(check_outpt(self.label[self.current_env].unsqueeze(0), self.traj.unsqueeze(0), tol_neg=tol_neg, tol_pos=tol_pos))\n",
    "            return (state, reward, True, {})\n",
    "        else:\n",
    "            return (state, 0., False, {})\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def render(self, mode):\n",
    "        pass\n",
    "\n",
    "class toy_exper_model(OnPolicyAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            policy: Union[str, Type[ActorCriticPolicy]] = 'MlpPolicy',\n",
    "            env: Union[GymEnv, str] = None,\n",
    "            learning_rate: Union[float, Schedule] = 3e-4,\n",
    "            n_steps: int = 2048,\n",
    "            batch_size: int = 64,\n",
    "            n_epochs: int = 10,\n",
    "            gamma: float = 0.99,\n",
    "            gae_lambda: float = 0.95,\n",
    "            clip_range: Union[float, Schedule] = 0.2,\n",
    "            clip_range_vf: Union[None, float, Schedule] = None,\n",
    "            normalize_advantage: bool = True,\n",
    "            ent_coef: float = 0.0,\n",
    "            vf_coef: float = 0.5,\n",
    "            max_grad_norm: float = 0.5,\n",
    "            use_sde: bool = False,\n",
    "            sde_sample_freq: int = -1,\n",
    "            target_kl: Optional[float] = None,\n",
    "            tensorboard_log: Optional[str] = None,\n",
    "            create_eval_env: bool = False,\n",
    "            policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            verbose: int = 0,\n",
    "            seed: Optional[int] = None,\n",
    "            device: Union[th.device, str] = \"auto\",\n",
    "            _init_setup_model: bool = True,\n",
    "            train_data = None\n",
    "        ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            create_eval_env=create_eval_env,\n",
    "            seed=seed,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "        self.data = train_data.data\n",
    "        self.label = train_data.label\n",
    "        #obs = step, data, action, current_env\n",
    "        self.observation_space = gym.spaces.box.Box(np.array([0, -2,-2,-2,-2, 0,0,0,0.,0]), np.array([6, 2,2,2,2, 1,1,1,1.,train_data.data.size(0)]), (10,), float)\n",
    "        #next state (4)\n",
    "        self.action_space = gym.spaces.box.Box(np.array([0,0,0,0]), np.array([1,1,1,1]), (4,), float)\n",
    "\n",
    "    def predict(self, obs, state=None, episode_start=None, deterministic=False):\n",
    "        step = int(obs.reshape(-1)[0])\n",
    "        env = int(obs.reshape(-1)[-1])\n",
    "        #print(f'expert: {self.label[env, step].reshape(1, -1).shape}')\n",
    "        return self.label[env, step].reshape(1, -1), self.label[env, step].reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_env = my_env(train_data=train_data)\n",
    "val_env = my_env(train_data=val_data)\n",
    "my_expert = toy_exper_model(train_data=train_data, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions():\n",
    "    expert = my_expert\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(toy_env)]),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=10000),\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = sample_expert_transitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Simon Stepputtis <sstepput@asu.edu>, Interactive Robotics Lab, Arizona State University\n",
    "\n",
    "from pickle import NONE\n",
    "from urllib.parse import non_hierarchical\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from hashids import Hashids\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class TBoardGraphsTorch():\n",
    "    def __init__(self, logname= None, data_path = None):\n",
    "        if logname is not None:\n",
    "            self.__hashids           = Hashids()\n",
    "            #self.logdir              = \"Data/TBoardLog/\" + logname + \"/\"\n",
    "            self.logdir              = os.path.join(data_path, \"gboard/\" + logname + \"/\")\n",
    "            print(f'log dir: {self.logdir + \"train/\"}')\n",
    "            self.__tboard_train      = tf.summary.create_file_writer(self.logdir + \"train/\")\n",
    "            self.__tboard_validation = tf.summary.create_file_writer(self.logdir + \"validate/\")\n",
    "            #self.voice               = Voice(path=data_path)\n",
    "        self.fig, self.ax = plt.subplots(3,3)\n",
    "\n",
    "    def startDebugger(self):\n",
    "        tf.summary.trace_on(graph=True, profiler=True)\n",
    "    \n",
    "    def stopDebugger(self):\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=self.logdir)\n",
    "\n",
    "    def finishFigure(self, fig):\n",
    "        fig.canvas.draw()\n",
    "        data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        return data\n",
    "    \n",
    "    def addTrainScalar(self, name, value, stepid):\n",
    "        with self.__tboard_train.as_default():\n",
    "            tfvalue = self.torch2tf(value)\n",
    "            tf.summary.scalar(name, tfvalue, step=stepid)\n",
    "\n",
    "    def addValidationScalar(self, name, value, stepid):\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tfvalue = self.torch2tf(value)\n",
    "            tf.summary.scalar(name, tfvalue, step=stepid)\n",
    "\n",
    "    def torch2tf(self, inpt):\n",
    "        if inpt is not None:\n",
    "            return tf.convert_to_tensor(inpt.detach().cpu().numpy())\n",
    "        else:\n",
    "            return inpt\n",
    "\n",
    "    def plotTrajectory(self, y_true, y_pred, dt_true, dt_pred, stepid):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "        tf_dt_true = self.torch2tf(dt_true)\n",
    "        tf_dt_pred = self.torch2tf(dt_pred)\n",
    "\n",
    "        fig, ax = plt.subplots(3,3)\n",
    "        fig.set_size_inches(9, 9)\n",
    "\n",
    "        tf_dt_true = 1.0/tf_dt_true.numpy()\n",
    "        tf_dt_pred = 1.0/tf_dt_pred.numpy()[0]\n",
    "\n",
    "        max_trj_len = tf_y_true.shape[0]\n",
    "        for sp in range(7):\n",
    "            idx = sp // 3\n",
    "            idy = sp  % 3\n",
    "            ax[idx,idy].clear()\n",
    "            ax[idx,idy].plot(range(max_trj_len), tf_y_pred[:,sp], alpha=0.5, color='midnightblue')\n",
    "            ax[idx,idy].plot(range(max_trj_len), tf_y_true[:,sp], alpha=0.5, color='forestgreen')\n",
    "            # ax[idx,idy].plot([dt_pred, dt_pred], [-0.1, 1.1], alpha=0.5, linestyle=\":\", color=\"midnightblue\")\n",
    "            # ax[idx,idy].plot([dt_true, dt_true], [-0.1, 1.1], alpha=0.5, linestyle=\":\", color=\"forestgreen\")\n",
    "            # ax[idx,idy].set_ylim([-0.1, 1.1])\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Trajectory\", data=result, step=stepid)\n",
    "\n",
    "    def idToText(self, id):\n",
    "        names = [\"\", \"ysr\", \"rsr\", \"gsr\", \"bsr\", \"psr\", \"ylr\", \"rlr\", \"glr\", \"blr\", \"plr\", \"yss\", \"rss\", \"gss\", \"bss\", \"pss\", \"yls\", \"rls\", \"gls\", \"bls\", \"pls\"]\n",
    "        return names[id]\n",
    "\n",
    "    def plotImageRegions(self, image, image_dict, stepid):\n",
    "        # Visualization of the results of a detection.\n",
    "        num_detected = len([v for v in image_dict[\"detection_scores\"][0] if v > 0.5]) \n",
    "        image_np     = image.numpy()       \n",
    "        for i in range(num_detected):\n",
    "            ymin, xmin, ymax, xmax = image_dict['detection_boxes'][0][i,:]\n",
    "            pt1 = (int(xmin*image_np.shape[1]), int(ymin*image_np.shape[0]))\n",
    "            pt2 = (int(xmax*image_np.shape[1]), int(ymax*image_np.shape[0]))\n",
    "            image_np = cv2.rectangle(image_np, pt1, pt2, (255, 0, 0), 2)\n",
    "            image_np = cv2.putText(image_np, self.idToText(image_dict['detection_classes'][0][i]) + \" {:.1f}%\".format(image_dict[\"detection_scores\"][0][i] * 100), pt1, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(image_np)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Image\", data=result, step=stepid)\n",
    "\n",
    "    def plotAttention(self, attention_weights, image_dict, language, stepid):\n",
    "        tf_attention_weights = self.torch2tf(attention_weights)\n",
    "        tf_language = self.torch2tf(language)\n",
    "\n",
    "        tf_attention_weights = tf_attention_weights.numpy()\n",
    "        classes           = image_dict[\"detection_classes\"][0][:len(tf_attention_weights)].numpy().astype(dtype=np.int32)\n",
    "        classes           = [self.idToText(i) for i in classes]\n",
    "        x                 = np.arange(len(tf_attention_weights))\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.bar(x, tf_attention_weights)\n",
    "        plt.xticks(x, classes)\n",
    "        ax.set_ylim([0, 1])\n",
    "        plt.text(0.01, 0.95, self.voice.tokensToSentence(tf_language.numpy().tolist()), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Attention\", data=result, step=stepid)\n",
    "    \n",
    "    def plotClassAccuracy(self, gt_class, pred_class, pred_class_std, language, stepid):\n",
    "        labels     = [\"ysr\", \"rsr\", \"gsr\", \"bsr\", \"psr\", \"ylr\", \"rlr\", \"glr\", \"blr\", \"plr\", \"yss\", \"rss\", \"gss\", \"bss\", \"pss\", \"yls\", \"rls\", \"gls\", \"bls\", \"pls\"]\n",
    "        tf_gt_class = self.torch2tf(gt_class)\n",
    "        tf_pred_class = self.torch2tf(pred_class)\n",
    "        tf_language = self.torch2tf(language)\n",
    "\n",
    "        \n",
    "        \n",
    "        tf_gt_class   = tf_gt_class.numpy()\n",
    "        tf_pred_class = tf_pred_class.numpy()\n",
    "        x          = np.arange(len(tf_gt_class))\n",
    "        width      = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        #rects1 = ax.bar(x - width/2, gt_class, width, label='GT', color=\"forestgreen\")\n",
    "        #rects2 = ax.bar(x + width/2, pred_class, width, yerr=pred_class_std, label='Pred', color=\"midnightblue\")\n",
    "        ax.set_xticks(x)\n",
    "        # ax.set_xticklabels(labels)\n",
    "        plt.text(0.01, 0.95, self.voice.tokensToSentence(tf_language.numpy().tolist()), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Attention\", data=result, step=stepid)\n",
    "\n",
    "    def plotDeltaT(self, y_true, y_pred, stepid):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "\n",
    "        gt = tf_y_true.numpy()\n",
    "        pd = tf_y_pred.numpy()[:,0]\n",
    "        jdata = np.stack((gt,pd), axis=1)\n",
    "        svals = jdata[np.argsort(jdata[:,0]),:]\n",
    "        x     = np.arange(svals.shape[0])\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(x - width/2, svals[:,0], width, label='GT', color=\"forestgreen\")\n",
    "        rects2 = ax.bar(x + width/2, svals[:,1], width, label='Pred', color=\"midnightblue\")\n",
    "        ax.set_xticks(x)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"DeltaT\", data=result, step=stepid)\n",
    "\n",
    "    def plotWeights(self, gt_w, pred_w, stepid):\n",
    "        tf_gt_w = self.torch2tf(gt_w)\n",
    "        tf_pred_w = self.torch2tf(pred_w)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2,sharey=True,sharex=True)\n",
    "        # fig.set_size_inches(4, 10)\n",
    "\n",
    "        combined_weights = np.concatenate((tf_gt_w.numpy(), tf_pred_w.numpy()), axis=0).T\n",
    "\n",
    "        ax1.imshow(combined_weights[:,:7], cmap=\"RdBu\")\n",
    "        ax2.imshow(combined_weights[:,7:], cmap=\"RdBu\")\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Weights\", data=result, step=stepid)\n",
    "\n",
    "    def interpolateTrajectory(self, trj, target):\n",
    "        tf_trj = self.torch2tf(trj)\n",
    "        tf_target = self.torch2tf(target)\n",
    "\n",
    "        current_length = tf_trj.shape[0]\n",
    "        dimensions     = tf_trj.shape[1]\n",
    "        result         = np.zeros((tf_target, dimensions), dtype=np.float32)\n",
    "    \n",
    "        for i in range(dimensions):\n",
    "            result[:,i] = np.interp(np.linspace(0.0, 1.0, num=tf_target), np.linspace(0.0, 1.0, num=current_length), trj[:,i])\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def plotDMPTrajectory(self, y_true, y_pred, y_pred_std = None, phase= None, \\\n",
    "        dt= None, p_dt= None, stepid= None, name = \"Trajectory\", save = False, \\\n",
    "            name_plot = None, path=None, tol_neg = None, tol_pos=None, inpt = None, opt_gen_trj=None, window = 0):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "        tf_phase = self.torch2tf(phase)\n",
    "        tf_inpt = self.torch2tf(inpt)\n",
    "        if p_dt is not None:\n",
    "            tf_dt = self.torch2tf(dt)\n",
    "            tf_p_dt = self.torch2tf(p_dt)\n",
    "        if opt_gen_trj is not None:\n",
    "            tf_opt_gen_trj = self.torch2tf(opt_gen_trj)\n",
    "            tf_opt_gen_trj = tf_opt_gen_trj.numpy()\n",
    "\n",
    "        tf_y_true      = tf_y_true.numpy()\n",
    "        tf_y_pred      = tf_y_pred.numpy()\n",
    "        tf_inpt        = tf_inpt.numpy()\n",
    "        if tf_phase is not None:\n",
    "            tf_phase       = tf_phase.numpy()\n",
    "\n",
    "        if p_dt is not None:\n",
    "            tf_dt          = tf_dt.numpy() * 350.0\n",
    "            tf_p_dt        = tf_p_dt.numpy()\n",
    "        trj_len      = tf_y_true.shape[0]\n",
    "        \n",
    "        #fig, ax = plt.subplots(3,3)\n",
    "        fig, ax = self.fig, self.ax\n",
    "        #fig.set_size_inches(9, 9)\n",
    "        neg_inpt = tf_y_true + tol_neg[None,:].cpu().numpy()\n",
    "        pos_inpt = tf_y_true + tol_pos[None,:].cpu().numpy()\n",
    "        for sp in range(len(tf_y_true[0])):\n",
    "            idx = sp // 3\n",
    "            idy = sp  % 3\n",
    "            ax[idx,idy].clear()\n",
    "\n",
    "            # GT Trajectory:\n",
    "            if tol_neg is not None:\n",
    "\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), neg_inpt[:,sp], alpha=0.75, color='orangered')\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), pos_inpt[:,sp], alpha=0.75, color='orangered')\n",
    "            ax[idx,idy].plot(range(trj_len), tf_y_true[:,sp],   alpha=1.0, color='forestgreen')            \n",
    "            ax[idx,idy].plot(range(tf_y_pred.shape[0]), tf_y_pred[:,sp], alpha=0.75, color='mediumslateblue')\n",
    "            if opt_gen_trj is not None:\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), tf_opt_gen_trj[:,sp], alpha=0.75, color='lightseagreen')\n",
    "                diff_vec = tf_opt_gen_trj - tf_y_pred\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), diff_vec[:,sp], alpha=0.75, color='pink')\n",
    "\n",
    "            #ax[idx,idy].errorbar(range(tf_y_pred.shape[0]), tf_y_pred[:,sp], xerr=None, yerr=None, alpha=0.25, fmt='none', color='mediumslateblue')\n",
    "            #ax[idx,idy].set_ylim([-0.1, 1.1])\n",
    "            if p_dt is not None:\n",
    "                ax[idx,idy].plot([tf_dt, tf_dt], [0.0,1.0], linestyle=\":\", color='forestgreen')\n",
    "\n",
    "        if inpt is not None:\n",
    "            ax[-1,-1].clear()\n",
    "            ax[-1,-1].plot(range(inpt.shape[-1]), tf_inpt,   alpha=1.0, color='forestgreen')     \n",
    "        \n",
    "        if tf_phase is not None:\n",
    "            ax[2,2].clear()\n",
    "            ax[2,2].plot(range(tf_y_pred.shape[0]), tf_phase, color='orange')\n",
    "        if p_dt is not None:\n",
    "            ax[2,2].plot([tf_dt, tf_dt], [0.0,1.0], linestyle=\":\", color='forestgreen')\n",
    "            ax[2,2].plot([tf_p_dt*350.0, tf_p_dt*350.0], [0.0,1.0], linestyle=\":\", color='mediumslateblue')\n",
    "            ax[2,2].set_ylim([-0.1, 1.1])\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        if save:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            plt.savefig(path + name_plot + '.png')\n",
    "        #fig.clear()\n",
    "        #plt.close()\n",
    "        if not save:\n",
    "            with self.__tboard_validation.as_default():\n",
    "                tf.summary.image(name, data=result, step=stepid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashids import Hashids\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tboard = TBoardGraphsTorch(logname='asd', data_path='/home/hendrik/Documents/master_project/LokalData/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.policies import *\n",
    "policy = ActorCriticPolicy(observation_space=toy_env.observation_space, action_space=toy_env.action_space, lr_schedule=lambda _: torch.finfo(torch.float32).max, net_arch = [dict(pi=[200, 200], vf=[200, 200])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = bc.BC(\n",
    "    observation_space=toy_env.observation_space,\n",
    "    action_space=toy_env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    policy=policy,\n",
    "    device='cpu'\n",
    ")\n",
    "#bc_trainer.train(n_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from active_critic.utils.gym_utils import make_policy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.policy.active_critic_policy import ActiveCriticPolicy\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from metaworld.envs import \\\n",
    "    ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from metaworld.policies import *\n",
    "from gym.wrappers import TimeLimit\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from active_critic.utils.rollout import rollout, make_sample_until, flatten_trajectories\n",
    "from stable_baselines3.common.type_aliases import GymEnv\n",
    "from gym import Env\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "class reset_counter(gym.Wrapper):\n",
    "    def __init__(self, env: Env) -> None:\n",
    "        super().__init__(env)\n",
    "        self.reset_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.reset_count+=1\n",
    "        return super().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PositionalEncoding(d_model=14, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetCounterWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: Env) -> None:\n",
    "        super().__init__(env)\n",
    "        self.reset_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.reset_count+=1\n",
    "        return super().reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        obsv, rew, done, info = super().step(action)\n",
    "        return obsv, rew, done, info\n",
    "    \n",
    "class ImitationLearningWrapper:\n",
    "    def __init__(self, policy, env: GymEnv):\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        self.policy = policy\n",
    "\n",
    "    def predict(self, obsv, deterministic=None):\n",
    "        actions = []\n",
    "        for obs in obsv:\n",
    "            actions.append(self.policy.get_action(obs))\n",
    "        return actions\n",
    "    \n",
    "class NoLookupWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: Env, seq_len) -> None:\n",
    "        super().__init__(env)\n",
    "        self.obsv = None\n",
    "        self.seq_len = seq_len\n",
    "        self.current_step = 0\n",
    "        self.pe = None\n",
    "        self.succes = 0\n",
    "        self.not_grade = False\n",
    "        self.current_obsv = None\n",
    "\n",
    "    def reset(self):\n",
    "        obsv = super().reset()\n",
    "\n",
    "        if self.pe is None:\n",
    "            if (obsv.shape[-1] % 2 ) != 0:\n",
    "                self.not_grade = True\n",
    "                d_model = obsv.shape[-1] + 1\n",
    "            else:\n",
    "                d_model = obsv.shape[-1]\n",
    "            self.act_d_model = obsv.shape[-1]\n",
    "            self.pe = PositionalEncoding(d_model=d_model, dropout=0)\n",
    "        if self.not_grade:\n",
    "            n_obsv = np.concatenate((obsv, np.zeros_like(obsv)[...,:1]), axis=-1)\n",
    "        else:\n",
    "            n_obsv = obsv\n",
    "        self.obsv = th.tensor(n_obsv)\n",
    "        self.obsv = self.obsv.reshape([1, 1, -1]).repeat([1, self.seq_len, 1])\n",
    "        self.obsv = self.pe.forward(self.obsv)\n",
    "        self.current_obsv = obsv.reshape([1,-1])\n",
    "        return self.obsv[0, 0, :self.act_d_model].numpy()\n",
    "\n",
    "    def step(self, action):\n",
    "        print(action)\n",
    "        obsv, rew, done, info = super().step(action)\n",
    "        self.current_step += 1\n",
    "        if rew == 10:\n",
    "            self.succes = 10\n",
    "        self.current_obsv = obsv.reshape([1,-1])\n",
    "        #return self.obsv[0, self.current_step, :self.act_d_model].numpy(), self.succes, done, info\n",
    "        return obsv, self.succes, done, info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummy_vec_env(name, seq_len):\n",
    "    policy_dict = make_policy_dict()\n",
    "\n",
    "    env_tag = name\n",
    "    max_episode_steps = seq_len\n",
    "    env = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_tag][1]]()\n",
    "    env._freeze_rand_vec = False\n",
    "    reset_env = ResetCounterWrapper(env=env)\n",
    "    timelimit = TimeLimit(env=reset_env, max_episode_steps=max_episode_steps)\n",
    "    #sparse_lookup = NoLookupWrapper(timelimit, seq_len=seq_len)\n",
    "\n",
    "    dv1 = DummyVecEnv([lambda: RolloutInfoWrapper(timelimit)])\n",
    "    vec_expert = ImitationLearningWrapper(\n",
    "        policy=policy_dict[env_tag][0], env=dv1)\n",
    "    return dv1, vec_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env, expert = make_dummy_vec_env(name='pickplace', seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data import rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions(venv, expert):\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        venv,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=20),\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = sample_expert_transitions(venv=val_env, expert=expert.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env.step(np.array([[0,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions, rollouts = sample_expert_transitions(vec_expert.predict, env=env, num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(500):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=val_env.observation_space,\n",
    "    action_space=val_env.action_space,\n",
    "    demonstrations=transitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = TBoardGraphs(logname='BC pickplace 10', data_path='/data/bing/hendrik/gboard/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    succ, rew = asd(val_env, bc_trainer.policy)\n",
    "    print(f'succ: {succ.mean()}')\n",
    "    tboard.addValidationScalar('Success Rate', th.tensor(succ.mean()), stepid=i)\n",
    "    bc_trainer.train(n_epochs=1, log_interval=100000000000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TQC\n",
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqc = TQC.load('/home/hendrik/Documents/master_project/LokalData/push_tqc.zip')\n",
    "val_env, _ = make_dummy_vec_env(name='push', seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sucess, rew = asd(val_env, tqc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sucess[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "a = th.rand([4,3,1])\n",
    "b = a.squeeze().max(dim=-1).values\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TQC\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, n_quantiles=25)\n",
    "learner = TQC(\"MlpPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.use_alt_policy = False\n",
    "learner.alternative_policy = vec_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.tboard_graphs import TBoardGraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_net = BasicRewardNet(\n",
    "    env.observation_space, env.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n",
    "\n",
    "gail_trainer = gail.GAIL(\n",
    "    demonstrations=transitions,\n",
    "    demo_batch_size=1024,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=env,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.envs[0].reset_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gail_trainer.set_demonstrations(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    gail_trainer.train(1000)\n",
    "    _, next_rollouts = sample_expert_transitions(vec_expert.predict, env=env, num=100)\n",
    "    rollouts = rollouts + next_rollouts\n",
    "    transitions = rollout.flatten_trajectories(rollouts)\n",
    "    gail_trainer.set_demonstrations(transitions)\n",
    "    _, rews = asd(env=env, learner=learner)\n",
    "    print(f'rews: {rews.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rews = asd(env=env, learner=learner)\n",
    "print(f'rews: {rews.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rews).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rews).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gail_Trainer = gail.GAIL(demonstrations=transitions, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = TBoardGraphsTorch(logname='asd', data_path='/home/hendrik/Documents/master_project/LokalData/test/')\n",
    "for i in range(100):\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = bc_trainer.policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='imitation baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='imitation baseline', opt_gen_trj = None, window=None)\n",
    "    bc_trainer.train(n_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "vec_toy_env = make_vec_env(my_env, n_envs=1)from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n",
    "\n",
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save('backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.load('backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0\n",
    "rein_model = PPO(\"MlpPolicy\", vec_toy_env, verbose=0, learning_rate=1e-4)\n",
    "rein_model.policy = policy\n",
    "rein_model.policy.to(rein_model.device)\n",
    "\n",
    "for i in range(100):\n",
    "    rein_model.learn(total_timesteps=1000)\n",
    "    print_reward(rein_model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLED_ENVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_TAKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_reward(policy, logname = 'ppo'):\n",
    "    tboard = TBoardGraphsTorch(logname=logname, data_path='/home/hendrik/Documents/master_project/LokalData/stableBaselines/')\n",
    "    rew = []\n",
    "    for j in range(1000):from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n",
    "\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(f'num_envs: {SAMPLED_ENVS}')\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='ppo fine tuning baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='ppo fine tuning baseline', opt_gen_trj = None, window=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = bc_trainer.policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='imitation baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='imitation baseline', opt_gen_trj = None, window=None)\n",
    "    bc_trainer.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.stats\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvergenceDetector():\n",
    "    def __init__(self, model_setup, model_constructor) -> None:\n",
    "        self.model_constructor = model_constructor\n",
    "        self.loss_history = None\n",
    "        self.mean_loss_history = []\n",
    "\n",
    "    def add_loss(self, loss, setup, set_meta):\n",
    "        if self.loss_history is None:\n",
    "            self.loss_history = loss\n",
    "        else:\n",
    "            self.loss_history = torch.cat((self.loss_history, loss))\n",
    "        if self.detect_convergence():\n",
    "            setup()\n",
    "            set_meta()\n",
    "\n",
    "    def torch_compute_confidence_interval(self, data: Tensor,\n",
    "                                           confidence: float = 0.95\n",
    "                                           ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Computes the confidence interval for a given survey of a data set.\n",
    "        \"\"\"\n",
    "        n = len(data)\n",
    "        mean: Tensor = data.mean()\n",
    "        # se: Tensor = scipy.stats.sem(data)  # compute standard error\n",
    "        # se, mean: Tensor = torch.std_mean(data, unbiased=True)  # compute standard error\n",
    "        se: Tensor = data.std(unbiased=True) / (n**0.5)\n",
    "        t_p: float = float(scipy.stats.t.ppf((1 + confidence) / 2., n - 1))\n",
    "        ci = t_p * se\n",
    "        return mean, ci\n",
    "\n",
    "    def detect_convergence(self):\n",
    "        self.mean_loss_history.append(self.torch_compute_confidence_interval(data=self.loss_history))\n",
    "        if len(self.mean_loss_history) > 1:\n",
    "            converged = True\n",
    "            for mean_loss in self.mean_loss_history[-5:-1]:\n",
    "                if (mean_loss[0] > (self.mean_loss_history[-1][0] + self.mean_loss_history[-1][1])):\n",
    "                    converged = False\n",
    "            return converged\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = ConvergenceDetector(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.loss_history = torch.randn(1000)\n",
    "print(cd.detect_convergence())\n",
    "print(cd.detect_convergence())\n",
    "cd.loss_history = torch.randn(1000) - 1\n",
    "print('_______________')\n",
    "print(cd.detect_convergence())\n",
    "print(cd.mean_loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.mean_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
