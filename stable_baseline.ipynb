{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "learner.use_alt_policy = False\n",
    "learner.alternative_policy = vec_expert\n",
    "tboard = TBoardGraphs(logname='TQC without expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(5000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n",
    "    print(env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "import argparse\n",
    "from sb3_contrib import TQC\n",
    "import copy\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "def run_experiment(device):\n",
    "\n",
    "    env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "    val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "    transitions, _ = sample_expert_transitions(vec_expert.predict, env, 100)\n",
    "    env.envs[0].reset_count = 0\n",
    "\n",
    "    reward_net = BasicRewardNet(\n",
    "        env.observation_space, env.action_space, normalize_input_layer=RunningNorm)\n",
    "\n",
    "    policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "    learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05, device=device)\n",
    "    \n",
    "    gail_trainer = gail.GAIL(\n",
    "        demonstrations=transitions,\n",
    "        demo_batch_size=1024,\n",
    "        gen_replay_buffer_capacity=2048,\n",
    "        n_disc_updates_per_round=4,\n",
    "        venv=env,\n",
    "        gen_algo=learner,\n",
    "        reward_net=reward_net\n",
    "\n",
    "    )\n",
    "    tboard = TBoardGraphs(logname='GAIL + TQC pickplace 100 pure', data_path='/data/bing/hendrik/gboard/')\n",
    "    current_step = 0\n",
    "    for i in range(10000):\n",
    "        gail_trainer.train(1000)\n",
    "        current_step += 5\n",
    "        succ, rew = asd(env=val_env, learner=learner)\n",
    "        print('____________________________________________')\n",
    "        print(f'succ: {succ.mean()}')\n",
    "        print(f'rew: {rew.mean()}')\n",
    "        tboard.addTrainScalar('Reward', value=th.tensor(rew.mean()), stepid=current_step)\n",
    "        tboard.addTrainScalar('Success Rate', value=th.tensor(succ.mean()), stepid=current_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "transitions, rollouts = sample_expert_transitions(vec_expert.predict, env, 10)\n",
    "env.envs[0].reset_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.model_src.transformer import PositionalEncoding\n",
    "positional_encoding = PositionalEncoding(d_model=10, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pomdp_rollouts(rollouts, lookup_frq, count_dim):\n",
    "    inpt = th.zeros([1, rollouts[0].obs.shape[0], count_dim])\n",
    "    positional_encoding = PositionalEncoding(d_model=10, dropout=0)\n",
    "    pe = positional_encoding.forward(inpt).numpy()\n",
    "    for ro in rollouts:\n",
    "        for i in range(ro.obs.shape[0]):\n",
    "            if i % lookup_frq == 0:\n",
    "                obsv = copy.deepcopy(ro.obs[i])\n",
    "            else:\n",
    "                ro.obs[i] = copy.deepcopy(obsv)\n",
    "        ro.obs[:, 20:20+count_dim] = pe\n",
    "        if i % lookup_frq == 0:\n",
    "            obs = ro\n",
    "    return rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp_rollouts = make_pomdp_rollouts(rollouts, 1, count_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pomdp_transitions = rollout.flatten_trajectories(pomdp_rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    env.observation_space, env.action_space, normalize_input_layer=RunningNorm)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05, device=device)\n",
    "\n",
    "gail_trainer = gail.GAIL(\n",
    "        demonstrations=pomdp_transitions,\n",
    "        demo_batch_size=1024,\n",
    "        gen_replay_buffer_capacity=2048,\n",
    "        n_disc_updates_per_round=4,\n",
    "        venv=env,\n",
    "        gen_algo=learner,\n",
    "        reward_net=reward_net\n",
    "\n",
    "    )\n",
    "tboard = TBoardGraphs(logname='GAIL + TQC pickplace 100 pomdp', data_path='/data/bing/hendrik/gboard/')\n",
    "current_step = 0\n",
    "for i in range(10000):\n",
    "    gail_trainer.train(1000)\n",
    "    current_step += 5\n",
    "    succ, rew = asd(env=val_env, learner=learner)\n",
    "    print('____________________________________________')\n",
    "    print(f'succ: {succ.mean()}')\n",
    "    print(f'rew: {rew.mean()}')\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rew.mean()), stepid=current_step)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(succ.mean()), stepid=current_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "import seals  # needed to load environments\n",
    "\n",
    "env = gym.make(\"seals/CartPole-v0\")\n",
    "expert = PPO(\n",
    "    policy=MlpPolicy,\n",
    "    env=env,\n",
    "    seed=0,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    n_steps=64,\n",
    ")\n",
    "expert.learn(1000)  # Note: set to 100000 to train a proficient expert\n",
    "\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "rollouts = rollout.rollout(\n",
    "    expert,\n",
    "    make_vec_env(\n",
    "        \"seals/CartPole-v0\",\n",
    "        n_envs=5,\n",
    "        post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],\n",
    "    ),\n",
    "    rollout.make_sample_until(min_timesteps=None, min_episodes=60),\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import gym\n",
    "\n",
    "\n",
    "venv = make_vec_env(\"seals/CartPole-v0\", n_envs=8)\n",
    "learner = PPO(\n",
    "    env=venv,\n",
    "    policy=MlpPolicy,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    ")\n",
    "reward_net = BasicRewardNet(\n",
    "    venv.observation_space, venv.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "gail_trainer = GAIL(\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=1024,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=venv,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")\n",
    "\n",
    "learner_rewards_before_training, _ = evaluate_policy(\n",
    "    learner, venv, 100, return_episode_rewards=True\n",
    ")\n",
    "gail_trainer.train(20000)  # Note: set to 300000 for better results\n",
    "learner_rewards_after_training, _ = evaluate_policy(\n",
    "    learner, venv, 100, return_episode_rewards=True\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(np.mean(learner_rewards_after_training))\n",
    "print(np.mean(learner_rewards_before_training))\n",
    "\n",
    "plt.hist(\n",
    "    [learner_rewards_before_training, learner_rewards_after_training],\n",
    "    label=[\"untrained\", \"trained\"],\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDP_Wrapper(gym.Wrapper):\n",
    "    def __init__(self, env, lookup_freq, pe_dim, seq_len) -> None:\n",
    "        super().__init__(env)\n",
    "        inpt = th.zeros([1, seq_len, pe_dim])\n",
    "\n",
    "        self.pe = positional_encoding.forward(inpt).numpy()\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        obsv =  super().reset()\n",
    "        obsv[20:30] = self.pe[0, 0]\n",
    "        self.current_step = 0\n",
    "        self.current_obv = np.copy(obsv)\n",
    "        return obsv\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        obsv, rew, done, info = super().step(action)\n",
    "        \n",
    "        obsv = np.copy(self.current_obv)\n",
    "        obsv[20:30] = self.pe[0, self.current_step]\n",
    "\n",
    "        return obsv, rew, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_policy_dict, ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE, ResetCounterWrapper, TimeLimit, StrictSeqLenWrapper, ImitationLearningWrapper\n",
    "\n",
    "def make_dummy_vec_env_pomdp(name, seq_len):\n",
    "    policy_dict = make_policy_dict()\n",
    "\n",
    "    env_tag = name\n",
    "    max_episode_steps = seq_len\n",
    "    env = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_tag][1]]()\n",
    "    env._freeze_rand_vec = False\n",
    "    reset_env = ResetCounterWrapper(env=env)\n",
    "    timelimit = TimeLimit(env=reset_env, max_episode_steps=max_episode_steps)\n",
    "    strict_time = StrictSeqLenWrapper(timelimit, seq_len=seq_len + 1)\n",
    "    pomdp = POMDP_Wrapper(env=strict_time, lookup_freq=1, pe_dim=10, seq_len=201)\n",
    "\n",
    "    dv1 = DummyVecEnv([lambda: RolloutInfoWrapper(pomdp)])\n",
    "    vec_expert = ImitationLearningWrapper(\n",
    "        policy=policy_dict[env_tag][0], env=dv1)\n",
    "    return dv1, vec_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env, vec_expert = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env_pomdp(name='pickplace', seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00615235, 0.6001898 , 0.19430117, 1.        , 0.00342816,\n",
       "        0.6789412 , 0.02      , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00604339, 0.5998951 ,\n",
       "        0.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "        1.        , 0.        , 1.        , 0.        , 1.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04631111, 0.85237634, 0.17972149]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6.1523519e-03, 6.0018981e-01, 1.9430117e-01, 1.0000000e+00,\n",
       "         3.4281577e-03, 6.7894119e-01, 2.0000000e-02, 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00, 6.0433885e-03, 5.9989512e-01,\n",
       "         8.4147096e-01, 5.4030234e-01, 1.5782663e-01, 9.8746681e-01,\n",
       "         2.5116224e-02, 9.9968451e-01, 3.9810613e-03, 9.9999207e-01,\n",
       "         6.3095731e-04, 9.9999982e-01, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         4.6311114e-02, 8.5237634e-01, 1.7972149e-01]], dtype=float32),\n",
       " array([0.00643198], dtype=float32),\n",
       " array([False]),\n",
       " [{'success': 0.0,\n",
       "   'near_object': 0.0,\n",
       "   'grasp_success': 0.0,\n",
       "   'grasp_reward': 0.006674097416744883,\n",
       "   'in_place_reward': 0.15060142424831546,\n",
       "   'obj_to_target': 0.23970905102239906,\n",
       "   'unscaled_reward': 0.006431983414570289}])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_env.step(np.array([[1,1,1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env.step(np.array([[1,1,1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00568 |\n",
      "|    entropy        | 5.68     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 64.5     |\n",
      "|    loss           | 4.32     |\n",
      "|    neglogp        | 4.33     |\n",
      "|    prob_true_act  | 0.0142   |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "474batch [00:01, 272.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | -0.00365 |\n",
      "|    entropy        | 3.65     |\n",
      "|    epoch          | 8        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 92.2     |\n",
      "|    loss           | 1.84     |\n",
      "|    neglogp        | 1.85     |\n",
      "|    prob_true_act  | 0.166    |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "994batch [00:03, 277.79batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1000     |\n",
      "|    ent_loss       | -0.00174 |\n",
      "|    entropy        | 1.74     |\n",
      "|    epoch          | 16       |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 108      |\n",
      "|    loss           | 0.0619   |\n",
      "|    neglogp        | 0.0636   |\n",
      "|    prob_true_act  | 1.07     |\n",
      "|    samples_so_far | 32032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240batch [00:04, 276.01batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 0         |\n",
      "|    ent_loss       | -0.000903 |\n",
      "|    entropy        | 0.903     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 118       |\n",
      "|    loss           | -0.568    |\n",
      "|    neglogp        | -0.567    |\n",
      "|    prob_true_act  | 2.22      |\n",
      "|    samples_so_far | 32        |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "476batch [00:01, 270.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | 0.000627 |\n",
      "|    entropy        | -0.627   |\n",
      "|    epoch          | 8        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 140      |\n",
      "|    loss           | -2.03    |\n",
      "|    neglogp        | -2.03    |\n",
      "|    prob_true_act  | 10.7     |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "987batch [00:03, 300.62batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1000     |\n",
      "|    ent_loss       | 0.00176  |\n",
      "|    entropy        | -1.76    |\n",
      "|    epoch          | 16       |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 157      |\n",
      "|    loss           | -2.69    |\n",
      "|    neglogp        | -2.69    |\n",
      "|    prob_true_act  | 30.5     |\n",
      "|    samples_so_far | 32032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240batch [00:04, 283.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | 0.00214  |\n",
      "|    entropy        | -2.14    |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 165      |\n",
      "|    loss           | -2.94    |\n",
      "|    neglogp        | -2.94    |\n",
      "|    prob_true_act  | 40       |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "497batch [00:01, 288.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | 0.00279  |\n",
      "|    entropy        | -2.79    |\n",
      "|    epoch          | 8        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 178      |\n",
      "|    loss           | -3.54    |\n",
      "|    neglogp        | -3.55    |\n",
      "|    prob_true_act  | 92.8     |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "979batch [00:03, 266.20batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1000     |\n",
      "|    ent_loss       | 0.00318  |\n",
      "|    entropy        | -3.18    |\n",
      "|    epoch          | 16       |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 189      |\n",
      "|    loss           | -4.35    |\n",
      "|    neglogp        | -4.35    |\n",
      "|    prob_true_act  | 127      |\n",
      "|    samples_so_far | 32032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240batch [00:04, 277.44batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | 0.00335  |\n",
      "|    entropy        | -3.35    |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 194      |\n",
      "|    loss           | -4.61    |\n",
      "|    neglogp        | -4.61    |\n",
      "|    prob_true_act  | 164      |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "492batch [00:01, 268.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | 0.00364  |\n",
      "|    entropy        | -3.64    |\n",
      "|    epoch          | 8        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 204      |\n",
      "|    loss           | -3.78    |\n",
      "|    neglogp        | -3.78    |\n",
      "|    prob_true_act  | 171      |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "985batch [00:03, 268.64batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1000     |\n",
      "|    ent_loss       | 0.00388  |\n",
      "|    entropy        | -3.88    |\n",
      "|    epoch          | 16       |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 213      |\n",
      "|    loss           | -2.92    |\n",
      "|    neglogp        | -2.92    |\n",
      "|    prob_true_act  | 269      |\n",
      "|    samples_so_far | 32032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240batch [00:04, 262.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | 0.00398  |\n",
      "|    entropy        | -3.98    |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 216      |\n",
      "|    loss           | -4.63    |\n",
      "|    neglogp        | -4.63    |\n",
      "|    prob_true_act  | 245      |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "495batch [00:01, 264.74batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | 0.00416  |\n",
      "|    entropy        | -4.16    |\n",
      "|    epoch          | 8        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 224      |\n",
      "|    loss           | -2.7     |\n",
      "|    neglogp        | -2.71    |\n",
      "|    prob_true_act  | 299      |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "976batch [00:03, 269.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1000     |\n",
      "|    ent_loss       | 0.00431  |\n",
      "|    entropy        | -4.31    |\n",
      "|    epoch          | 16       |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 230      |\n",
      "|    loss           | -4.56    |\n",
      "|    neglogp        | -4.56    |\n",
      "|    prob_true_act  | 256      |\n",
      "|    samples_so_far | 32032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240batch [00:04, 261.43batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | 0.00438  |\n",
      "|    entropy        | -4.38    |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 234      |\n",
      "|    loss           | -5.29    |\n",
      "|    neglogp        | -5.3     |\n",
      "|    prob_true_act  | 403      |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "487batch [00:01, 293.65batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | 0.0045   |\n",
      "|    entropy        | -4.5     |\n",
      "|    epoch          | 8        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 240      |\n",
      "|    loss           | -5.63    |\n",
      "|    neglogp        | -5.64    |\n",
      "|    prob_true_act  | 476      |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "986batch [00:03, 302.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1000     |\n",
      "|    ent_loss       | 0.00459  |\n",
      "|    entropy        | -4.59    |\n",
      "|    epoch          | 16       |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 245      |\n",
      "|    loss           | -4.93    |\n",
      "|    neglogp        | -4.93    |\n",
      "|    prob_true_act  | 497      |\n",
      "|    samples_so_far | 32032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240batch [00:04, 282.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | 0.00464  |\n",
      "|    entropy        | -4.64    |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 248      |\n",
      "|    loss           | -4.47    |\n",
      "|    neglogp        | -4.48    |\n",
      "|    prob_true_act  | 410      |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "484batch [00:01, 257.52batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 500      |\n",
      "|    ent_loss       | 0.00471  |\n",
      "|    entropy        | -4.71    |\n",
      "|    epoch          | 8        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 254      |\n",
      "|    loss           | -6.17    |\n",
      "|    neglogp        | -6.17    |\n",
      "|    prob_true_act  | 628      |\n",
      "|    samples_so_far | 16032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999batch [00:03, 271.15batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 1000     |\n",
      "|    ent_loss       | 0.0048   |\n",
      "|    entropy        | -4.8     |\n",
      "|    epoch          | 16       |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 260      |\n",
      "|    loss           | -3.98    |\n",
      "|    neglogp        | -3.99    |\n",
      "|    prob_true_act  | 644      |\n",
      "|    samples_so_far | 32032    |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1240batch [00:04, 275.07batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [60], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10000\u001b[39m):\n\u001b[1;32m     22\u001b[0m     bc_trainer\u001b[39m.\u001b[39mtrain(n_epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m     success, rews \u001b[39m=\u001b[39m asd(env\u001b[39m=\u001b[39;49mval_env, learner\u001b[39m=\u001b[39;49mbc_trainer\u001b[39m.\u001b[39;49mpolicy)\n\u001b[1;32m     24\u001b[0m     tboard\u001b[39m.\u001b[39maddTrainScalar(\u001b[39m'\u001b[39m\u001b[39mReward\u001b[39m\u001b[39m'\u001b[39m, value\u001b[39m=\u001b[39mth\u001b[39m.\u001b[39mtensor(rews\u001b[39m.\u001b[39mmean()), stepid\u001b[39m=\u001b[39mi)\n\u001b[1;32m     25\u001b[0m     tboard\u001b[39m.\u001b[39maddTrainScalar(\u001b[39m'\u001b[39m\u001b[39mSuccess Rate\u001b[39m\u001b[39m'\u001b[39m, value\u001b[39m=\u001b[39mth\u001b[39m.\u001b[39mtensor(success\u001b[39m.\u001b[39mmean()), stepid\u001b[39m=\u001b[39mi)\n",
      "Cell \u001b[0;32mIn [8], line 51\u001b[0m, in \u001b[0;36masd\u001b[0;34m(env, learner)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m     50\u001b[0m     action, _ \u001b[39m=\u001b[39m learner\u001b[39m.\u001b[39mpredict(obs)\n\u001b[0;32m---> 51\u001b[0m     obs, rew, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     52\u001b[0m     rews\u001b[39m.\u001b[39mappend(rew)\n\u001b[1;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m info[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msuccess\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     45\u001b[0m         )\n\u001b[1;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/imitation/data/wrappers.py:185\u001b[0m, in \u001b[0;36mRolloutInfoWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 185\u001b[0m     obs, rew, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    186\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_obs\u001b[39m.\u001b[39mappend(obs)\n\u001b[1;32m    187\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rews\u001b[39m.\u001b[39mappend(rew)\n",
      "Cell \u001b[0;32mIn [9], line 18\u001b[0m, in \u001b[0;36mPOMDP_Wrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     17\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     obsv, rew, done, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     20\u001b[0m     obsv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_obv)\n\u001b[1;32m     21\u001b[0m     obsv[\u001b[39m20\u001b[39m:\u001b[39m30\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpe[\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step]\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/gym/core.py:289\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/utils/gym_utils.py:127\u001b[0m, in \u001b[0;36mStrictSeqLenWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m     obsv, rew, done, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    129\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m obsv, rew, done, info\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/gym/core.py:289\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/gym/wrappers/time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/master_project/Code/active_critic/src/active_critic/utils/gym_utils.py:111\u001b[0m, in \u001b[0;36mResetCounterWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 111\u001b[0m     obsv, rew, done, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m obsv, rew, done, info\n",
      "File \u001b[0;32m~/anaconda3/envs/ac/lib/python3.10/site-packages/gym/core.py:289\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/Documents/master_project/Code/MetaWorld/metaworld/envs/mujoco/mujoco_env.py:25\u001b[0m, in \u001b[0;36m_assert_task_is_set.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m env\u001b[39m.\u001b[39m_set_task_called:\n\u001b[1;32m     21\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     22\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mYou must call env.set_task before using env.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     23\u001b[0m         \u001b[39m+\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[0;32m---> 25\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/master_project/Code/MetaWorld/metaworld/envs/mujoco/sawyer_xyz/sawyer_xyz_env.py:407\u001b[0m, in \u001b[0;36mSawyerXYZEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39m@_assert_task_is_set\u001b[39m\n\u001b[1;32m    405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    406\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_xyz_action(action[:\u001b[39m3\u001b[39m])\n\u001b[0;32m--> 407\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_simulation([action[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], \u001b[39m-\u001b[39;49maction[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]])\n\u001b[1;32m    408\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurr_path_length \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    410\u001b[0m     \u001b[39m# Running the simulator can sometimes mess up site positions, so\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     \u001b[39m# re-position them here to make sure they're accurate\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/master_project/Code/MetaWorld/metaworld/envs/mujoco/mujoco_env.py:116\u001b[0m, in \u001b[0;36mMujocoEnv.do_simulation\u001b[0;34m(self, ctrl, n_frames)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_frames):\n\u001b[1;32m    115\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msim\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    117\u001b[0m     \u001b[39mexcept\u001b[39;00m mujoco_py\u001b[39m.\u001b[39mMujocoException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    118\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39mstr\u001b[39m(err), category\u001b[39m=\u001b[39m\u001b[39mRuntimeWarning\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    ")\n",
    "\n",
    "tboard = TBoardGraphs(logname='BC pickplace 10', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    bc_trainer.train(n_epochs=20)\n",
    "    success, rews = asd(env=val_env, learner=bc_trainer.policy)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=i)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=i)\n",
    "    print(env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "reward_net = BasicRewardNet(\n",
    "    env.observation_space, env.action_space, normalize_input_layer=RunningNorm)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05, device=device)\n",
    "\n",
    "gail_trainer = gail.GAIL(\n",
    "    demonstrations=transitions,\n",
    "    demo_batch_size=1024,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=env,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions.obsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_policy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class asdasd(torch.utils.data.Dataset):\n",
    "    def __init__(self, asd):\n",
    "        print('init')\n",
    "        self.data = torch.arange(10)\n",
    "        print(len(self.data))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        print(len(self.data))\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/hendrik/Documents/master_project/LokalData/metaworld/pick-place/training_data/'\n",
    "device = 'cuda'\n",
    "batch_size = 2\n",
    "#train_data = TorchDatasetMW(path=path, device=device)\n",
    "train_data = asdasd(asd=1)\n",
    "train_indices = torch.randperm(int(len(train_data)))\n",
    "train_indices = train_indices[:int(len(train_indices)*1)]\n",
    "print(len(train_data))\n",
    "train_data = torch.utils.data.Subset(train_data, [0])\n",
    "print(f'len(train_data): {len(train_data)}')\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '/home/hendrik/Documents/master_project/LokalData/metaworld/small/train/'\n",
    "path_validate = '/home/hendrik/Documents/master_project/LokalData/metaworld/small/val/'\n",
    "train_data = TorchDatasetMWToy(path=path_train, device='cpu')\n",
    "val_data = TorchDatasetMWToy(path=path_validate, device='cpu')\n",
    "print(train_data.data.shape)\n",
    "print(train_data.label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global SAMPLED_ENVS\n",
    "global STEPS_TAKEN\n",
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_env():\n",
    "    def __init__(self):\n",
    "        #obs = step, data, action, current_env\n",
    "        self.observation_space = gym.spaces.box.Box(np.array([0, -2,-2,-2,-2, 0,0,0,0.,0]), np.array([6, 2,2,2,2, 1,1,1,1.,train_data.data.size(0)]), (10,), float)\n",
    "        #next state (4)\n",
    "        self.action_space = gym.spaces.box.Box(np.array([0,0,0,0]), np.array([1,1,1,1]), (4,), float)\n",
    "        self.metadata = {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}\n",
    "        self.steps = 0\n",
    "        self.current_env = -1\n",
    "        self.data = train_data.data\n",
    "        self.label = train_data.label\n",
    "        self.traj = None\n",
    "        self.num_envs = 1\n",
    "    def reset(self):\n",
    "        global SAMPLED_ENVS\n",
    "        global STEPS_TAKEN\n",
    "        STEPS_TAKEN += 1\n",
    "        SAMPLED_ENVS += 1\n",
    "        self.traj = None\n",
    "        self.current_env = (self.current_env + 1)%len(self.data)\n",
    "        self.steps = 0\n",
    "        last_action = torch.zeros(4, dtype=float, device=self.data.device)\n",
    "        step = torch.tensor(self.steps, device=self.data.device)\n",
    "        current_env = torch.tensor(self.current_env, device=self.data.device)\n",
    "        data = self.data[self.current_env, 0]\n",
    "        #label = self.label[self.current_env,0]\n",
    "        state = torch.cat((step.view(1), data, last_action, current_env.view(1)), dim=0).numpy()\n",
    "        #print(f'reset: {state.shape}')\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        global STEPS_TAKEN\n",
    "        STEPS_TAKEN += 1\n",
    "        if type(action) is np.ndarray:\n",
    "            action = torch.tensor(action, device=self.data.device)\n",
    "        if self.traj is None:\n",
    "            self.traj = action.reshape(1,-1)\n",
    "        else:\n",
    "            self.traj = torch.cat((self.traj, action.reshape(1,-1)), dim=0)\n",
    "\n",
    "\n",
    "\n",
    "        self.steps += 1\n",
    "        step = torch.tensor(self.steps, device=self.data.device)\n",
    "        current_env = torch.tensor(self.current_env, device=self.data.device)\n",
    "\n",
    "        #label = self.label[self.current_env, self.current_step]\n",
    "        data = self.data[self.current_env, 0]\n",
    "\n",
    "        state = torch.cat((step.view(1), data, action.reshape(-1), current_env.view(1)), dim=0).numpy()\n",
    "        #print(f'step: {state.shape}')\n",
    "\n",
    "        if self.steps >= self.label.size(1):\n",
    "            tol_neg = -0.55*torch.ones([self.traj.size(-1)])\n",
    "            tol_pos = 0.7*torch.ones([self.traj.size(-1)])\n",
    "            reward = int(check_outpt(self.label[self.current_env].unsqueeze(0), self.traj.unsqueeze(0), tol_neg=tol_neg, tol_pos=tol_pos))\n",
    "            return (state, reward, True, {})\n",
    "        else:\n",
    "            return (state, 0., False, {})\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def render(self, mode):\n",
    "        pass\n",
    "\n",
    "class toy_exper_model(OnPolicyAlgorithm):\n",
    "    def __init__(\n",
    "            self,\n",
    "            policy: Union[str, Type[ActorCriticPolicy]] = 'MlpPolicy',\n",
    "            env: Union[GymEnv, str] = None,\n",
    "            learning_rate: Union[float, Schedule] = 3e-4,\n",
    "            n_steps: int = 2048,\n",
    "            batch_size: int = 64,\n",
    "            n_epochs: int = 10,\n",
    "            gamma: float = 0.99,\n",
    "            gae_lambda: float = 0.95,\n",
    "            clip_range: Union[float, Schedule] = 0.2,\n",
    "            clip_range_vf: Union[None, float, Schedule] = None,\n",
    "            normalize_advantage: bool = True,\n",
    "            ent_coef: float = 0.0,\n",
    "            vf_coef: float = 0.5,\n",
    "            max_grad_norm: float = 0.5,\n",
    "            use_sde: bool = False,\n",
    "            sde_sample_freq: int = -1,\n",
    "            target_kl: Optional[float] = None,\n",
    "            tensorboard_log: Optional[str] = None,\n",
    "            create_eval_env: bool = False,\n",
    "            policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "            verbose: int = 0,\n",
    "            seed: Optional[int] = None,\n",
    "            device: Union[th.device, str] = \"auto\",\n",
    "            _init_setup_model: bool = True,\n",
    "            train_data = None\n",
    "        ):\n",
    "        super().__init__(\n",
    "            policy,\n",
    "            env,\n",
    "            learning_rate=learning_rate,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            gae_lambda=gae_lambda,\n",
    "            ent_coef=ent_coef,\n",
    "            vf_coef=vf_coef,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            use_sde=use_sde,\n",
    "            sde_sample_freq=sde_sample_freq,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            verbose=verbose,\n",
    "            device=device,\n",
    "            create_eval_env=create_eval_env,\n",
    "            seed=seed,\n",
    "            _init_setup_model=False,\n",
    "            supported_action_spaces=(\n",
    "                spaces.Box,\n",
    "                spaces.Discrete,\n",
    "                spaces.MultiDiscrete,\n",
    "                spaces.MultiBinary,\n",
    "            ),\n",
    "        )\n",
    "        self.data = train_data.data\n",
    "        self.label = train_data.label\n",
    "        #obs = step, data, action, current_env\n",
    "        self.observation_space = gym.spaces.box.Box(np.array([0, -2,-2,-2,-2, 0,0,0,0.,0]), np.array([6, 2,2,2,2, 1,1,1,1.,train_data.data.size(0)]), (10,), float)\n",
    "        #next state (4)\n",
    "        self.action_space = gym.spaces.box.Box(np.array([0,0,0,0]), np.array([1,1,1,1]), (4,), float)\n",
    "\n",
    "    def predict(self, obs, state=None, episode_start=None, deterministic=False):\n",
    "        step = int(obs.reshape(-1)[0])\n",
    "        env = int(obs.reshape(-1)[-1])\n",
    "        #print(f'expert: {self.label[env, step].reshape(1, -1).shape}')\n",
    "        return self.label[env, step].reshape(1, -1), self.label[env, step].reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_env = my_env(train_data=train_data)\n",
    "val_env = my_env(train_data=val_data)\n",
    "my_expert = toy_exper_model(train_data=train_data, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions():\n",
    "    expert = my_expert\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = ro.rollout(\n",
    "        expert,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(toy_env)]),\n",
    "        ro.make_sample_until(min_timesteps=None, min_episodes=10000),\n",
    "    )\n",
    "    return ro.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = sample_expert_transitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @author Simon Stepputtis <sstepput@asu.edu>, Interactive Robotics Lab, Arizona State University\n",
    "\n",
    "from pickle import NONE\n",
    "from urllib.parse import non_hierarchical\n",
    "#matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from hashids import Hashids\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "class TBoardGraphsTorch():\n",
    "    def __init__(self, logname= None, data_path = None):\n",
    "        if logname is not None:\n",
    "            self.__hashids           = Hashids()\n",
    "            #self.logdir              = \"Data/TBoardLog/\" + logname + \"/\"\n",
    "            self.logdir              = os.path.join(data_path, \"gboard/\" + logname + \"/\")\n",
    "            print(f'log dir: {self.logdir + \"train/\"}')\n",
    "            self.__tboard_train      = tf.summary.create_file_writer(self.logdir + \"train/\")\n",
    "            self.__tboard_validation = tf.summary.create_file_writer(self.logdir + \"validate/\")\n",
    "            #self.voice               = Voice(path=data_path)\n",
    "        self.fig, self.ax = plt.subplots(3,3)\n",
    "\n",
    "    def startDebugger(self):\n",
    "        tf.summary.trace_on(graph=True, profiler=True)\n",
    "    \n",
    "    def stopDebugger(self):\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.trace_export(name=\"model_trace\", step=0, profiler_outdir=self.logdir)\n",
    "\n",
    "    def finishFigure(self, fig):\n",
    "        fig.canvas.draw()\n",
    "        data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "        data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        return data\n",
    "    \n",
    "    def addTrainScalar(self, name, value, stepid):\n",
    "        with self.__tboard_train.as_default():\n",
    "            tfvalue = self.torch2tf(value)\n",
    "            tf.summary.scalar(name, tfvalue, step=stepid)\n",
    "\n",
    "    def addValidationScalar(self, name, value, stepid):\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tfvalue = self.torch2tf(value)\n",
    "            tf.summary.scalar(name, tfvalue, step=stepid)\n",
    "\n",
    "    def torch2tf(self, inpt):\n",
    "        if inpt is not None:\n",
    "            return tf.convert_to_tensor(inpt.detach().cpu().numpy())\n",
    "        else:\n",
    "            return inpt\n",
    "\n",
    "    def plotTrajectory(self, y_true, y_pred, dt_true, dt_pred, stepid):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "        tf_dt_true = self.torch2tf(dt_true)\n",
    "        tf_dt_pred = self.torch2tf(dt_pred)\n",
    "\n",
    "        fig, ax = plt.subplots(3,3)\n",
    "        fig.set_size_inches(9, 9)\n",
    "\n",
    "        tf_dt_true = 1.0/tf_dt_true.numpy()\n",
    "        tf_dt_pred = 1.0/tf_dt_pred.numpy()[0]\n",
    "\n",
    "        max_trj_len = tf_y_true.shape[0]\n",
    "        for sp in range(7):\n",
    "            idx = sp // 3\n",
    "            idy = sp  % 3\n",
    "            ax[idx,idy].clear()\n",
    "            ax[idx,idy].plot(range(max_trj_len), tf_y_pred[:,sp], alpha=0.5, color='midnightblue')\n",
    "            ax[idx,idy].plot(range(max_trj_len), tf_y_true[:,sp], alpha=0.5, color='forestgreen')\n",
    "            # ax[idx,idy].plot([dt_pred, dt_pred], [-0.1, 1.1], alpha=0.5, linestyle=\":\", color=\"midnightblue\")\n",
    "            # ax[idx,idy].plot([dt_true, dt_true], [-0.1, 1.1], alpha=0.5, linestyle=\":\", color=\"forestgreen\")\n",
    "            # ax[idx,idy].set_ylim([-0.1, 1.1])\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Trajectory\", data=result, step=stepid)\n",
    "\n",
    "    def idToText(self, id):\n",
    "        names = [\"\", \"ysr\", \"rsr\", \"gsr\", \"bsr\", \"psr\", \"ylr\", \"rlr\", \"glr\", \"blr\", \"plr\", \"yss\", \"rss\", \"gss\", \"bss\", \"pss\", \"yls\", \"rls\", \"gls\", \"bls\", \"pls\"]\n",
    "        return names[id]\n",
    "\n",
    "    def plotImageRegions(self, image, image_dict, stepid):\n",
    "        # Visualization of the results of a detection.\n",
    "        num_detected = len([v for v in image_dict[\"detection_scores\"][0] if v > 0.5]) \n",
    "        image_np     = image.numpy()       \n",
    "        for i in range(num_detected):\n",
    "            ymin, xmin, ymax, xmax = image_dict['detection_boxes'][0][i,:]\n",
    "            pt1 = (int(xmin*image_np.shape[1]), int(ymin*image_np.shape[0]))\n",
    "            pt2 = (int(xmax*image_np.shape[1]), int(ymax*image_np.shape[0]))\n",
    "            image_np = cv2.rectangle(image_np, pt1, pt2, (255, 0, 0), 2)\n",
    "            image_np = cv2.putText(image_np, self.idToText(image_dict['detection_classes'][0][i]) + \" {:.1f}%\".format(image_dict[\"detection_scores\"][0][i] * 100), pt1, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(image_np)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Image\", data=result, step=stepid)\n",
    "\n",
    "    def plotAttention(self, attention_weights, image_dict, language, stepid):\n",
    "        tf_attention_weights = self.torch2tf(attention_weights)\n",
    "        tf_language = self.torch2tf(language)\n",
    "\n",
    "        tf_attention_weights = tf_attention_weights.numpy()\n",
    "        classes           = image_dict[\"detection_classes\"][0][:len(tf_attention_weights)].numpy().astype(dtype=np.int32)\n",
    "        classes           = [self.idToText(i) for i in classes]\n",
    "        x                 = np.arange(len(tf_attention_weights))\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.bar(x, tf_attention_weights)\n",
    "        plt.xticks(x, classes)\n",
    "        ax.set_ylim([0, 1])\n",
    "        plt.text(0.01, 0.95, self.voice.tokensToSentence(tf_language.numpy().tolist()), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Attention\", data=result, step=stepid)\n",
    "    \n",
    "    def plotClassAccuracy(self, gt_class, pred_class, pred_class_std, language, stepid):\n",
    "        labels     = [\"ysr\", \"rsr\", \"gsr\", \"bsr\", \"psr\", \"ylr\", \"rlr\", \"glr\", \"blr\", \"plr\", \"yss\", \"rss\", \"gss\", \"bss\", \"pss\", \"yls\", \"rls\", \"gls\", \"bls\", \"pls\"]\n",
    "        tf_gt_class = self.torch2tf(gt_class)\n",
    "        tf_pred_class = self.torch2tf(pred_class)\n",
    "        tf_language = self.torch2tf(language)\n",
    "\n",
    "        \n",
    "        \n",
    "        tf_gt_class   = tf_gt_class.numpy()\n",
    "        tf_pred_class = tf_pred_class.numpy()\n",
    "        x          = np.arange(len(tf_gt_class))\n",
    "        width      = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        #rects1 = ax.bar(x - width/2, gt_class, width, label='GT', color=\"forestgreen\")\n",
    "        #rects2 = ax.bar(x + width/2, pred_class, width, yerr=pred_class_std, label='Pred', color=\"midnightblue\")\n",
    "        ax.set_xticks(x)\n",
    "        # ax.set_xticklabels(labels)\n",
    "        plt.text(0.01, 0.95, self.voice.tokensToSentence(tf_language.numpy().tolist()), horizontalalignment='left', verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Attention\", data=result, step=stepid)\n",
    "\n",
    "    def plotDeltaT(self, y_true, y_pred, stepid):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "\n",
    "        gt = tf_y_true.numpy()\n",
    "        pd = tf_y_pred.numpy()[:,0]\n",
    "        jdata = np.stack((gt,pd), axis=1)\n",
    "        svals = jdata[np.argsort(jdata[:,0]),:]\n",
    "        x     = np.arange(svals.shape[0])\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        rects1 = ax.bar(x - width/2, svals[:,0], width, label='GT', color=\"forestgreen\")\n",
    "        rects2 = ax.bar(x + width/2, svals[:,1], width, label='Pred', color=\"midnightblue\")\n",
    "        ax.set_xticks(x)\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"DeltaT\", data=result, step=stepid)\n",
    "\n",
    "    def plotWeights(self, gt_w, pred_w, stepid):\n",
    "        tf_gt_w = self.torch2tf(gt_w)\n",
    "        tf_pred_w = self.torch2tf(pred_w)\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2,sharey=True,sharex=True)\n",
    "        # fig.set_size_inches(4, 10)\n",
    "\n",
    "        combined_weights = np.concatenate((tf_gt_w.numpy(), tf_pred_w.numpy()), axis=0).T\n",
    "\n",
    "        ax1.imshow(combined_weights[:,:7], cmap=\"RdBu\")\n",
    "        ax2.imshow(combined_weights[:,7:], cmap=\"RdBu\")\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        plt.close()\n",
    "        with self.__tboard_validation.as_default():\n",
    "            tf.summary.image(\"Weights\", data=result, step=stepid)\n",
    "\n",
    "    def interpolateTrajectory(self, trj, target):\n",
    "        tf_trj = self.torch2tf(trj)\n",
    "        tf_target = self.torch2tf(target)\n",
    "\n",
    "        current_length = tf_trj.shape[0]\n",
    "        dimensions     = tf_trj.shape[1]\n",
    "        result         = np.zeros((tf_target, dimensions), dtype=np.float32)\n",
    "    \n",
    "        for i in range(dimensions):\n",
    "            result[:,i] = np.interp(np.linspace(0.0, 1.0, num=tf_target), np.linspace(0.0, 1.0, num=current_length), trj[:,i])\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def plotDMPTrajectory(self, y_true, y_pred, y_pred_std = None, phase= None, \\\n",
    "        dt= None, p_dt= None, stepid= None, name = \"Trajectory\", save = False, \\\n",
    "            name_plot = None, path=None, tol_neg = None, tol_pos=None, inpt = None, opt_gen_trj=None, window = 0):\n",
    "        tf_y_true = self.torch2tf(y_true)\n",
    "        tf_y_pred = self.torch2tf(y_pred)\n",
    "        tf_phase = self.torch2tf(phase)\n",
    "        tf_inpt = self.torch2tf(inpt)\n",
    "        if p_dt is not None:\n",
    "            tf_dt = self.torch2tf(dt)\n",
    "            tf_p_dt = self.torch2tf(p_dt)\n",
    "        if opt_gen_trj is not None:\n",
    "            tf_opt_gen_trj = self.torch2tf(opt_gen_trj)\n",
    "            tf_opt_gen_trj = tf_opt_gen_trj.numpy()\n",
    "\n",
    "        tf_y_true      = tf_y_true.numpy()\n",
    "        tf_y_pred      = tf_y_pred.numpy()\n",
    "        tf_inpt        = tf_inpt.numpy()\n",
    "        if tf_phase is not None:\n",
    "            tf_phase       = tf_phase.numpy()\n",
    "\n",
    "        if p_dt is not None:\n",
    "            tf_dt          = tf_dt.numpy() * 350.0\n",
    "            tf_p_dt        = tf_p_dt.numpy()\n",
    "        trj_len      = tf_y_true.shape[0]\n",
    "        \n",
    "        #fig, ax = plt.subplots(3,3)\n",
    "        fig, ax = self.fig, self.ax\n",
    "        #fig.set_size_inches(9, 9)\n",
    "        neg_inpt = tf_y_true + tol_neg[None,:].cpu().numpy()\n",
    "        pos_inpt = tf_y_true + tol_pos[None,:].cpu().numpy()\n",
    "        for sp in range(len(tf_y_true[0])):\n",
    "            idx = sp // 3\n",
    "            idy = sp  % 3\n",
    "            ax[idx,idy].clear()\n",
    "\n",
    "            # GT Trajectory:\n",
    "            if tol_neg is not None:\n",
    "\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), neg_inpt[:,sp], alpha=0.75, color='orangered')\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), pos_inpt[:,sp], alpha=0.75, color='orangered')\n",
    "            ax[idx,idy].plot(range(trj_len), tf_y_true[:,sp],   alpha=1.0, color='forestgreen')            \n",
    "            ax[idx,idy].plot(range(tf_y_pred.shape[0]), tf_y_pred[:,sp], alpha=0.75, color='mediumslateblue')\n",
    "            if opt_gen_trj is not None:\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), tf_opt_gen_trj[:,sp], alpha=0.75, color='lightseagreen')\n",
    "                diff_vec = tf_opt_gen_trj - tf_y_pred\n",
    "                ax[idx,idy].plot(range(tf_y_pred.shape[0]), diff_vec[:,sp], alpha=0.75, color='pink')\n",
    "\n",
    "            #ax[idx,idy].errorbar(range(tf_y_pred.shape[0]), tf_y_pred[:,sp], xerr=None, yerr=None, alpha=0.25, fmt='none', color='mediumslateblue')\n",
    "            #ax[idx,idy].set_ylim([-0.1, 1.1])\n",
    "            if p_dt is not None:\n",
    "                ax[idx,idy].plot([tf_dt, tf_dt], [0.0,1.0], linestyle=\":\", color='forestgreen')\n",
    "\n",
    "        if inpt is not None:\n",
    "            ax[-1,-1].clear()\n",
    "            ax[-1,-1].plot(range(inpt.shape[-1]), tf_inpt,   alpha=1.0, color='forestgreen')     \n",
    "        \n",
    "        if tf_phase is not None:\n",
    "            ax[2,2].clear()\n",
    "            ax[2,2].plot(range(tf_y_pred.shape[0]), tf_phase, color='orange')\n",
    "        if p_dt is not None:\n",
    "            ax[2,2].plot([tf_dt, tf_dt], [0.0,1.0], linestyle=\":\", color='forestgreen')\n",
    "            ax[2,2].plot([tf_p_dt*350.0, tf_p_dt*350.0], [0.0,1.0], linestyle=\":\", color='mediumslateblue')\n",
    "            ax[2,2].set_ylim([-0.1, 1.1])\n",
    "\n",
    "        result = np.expand_dims(self.finishFigure(fig), 0)\n",
    "        if save:\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            plt.savefig(path + name_plot + '.png')\n",
    "        #fig.clear()\n",
    "        #plt.close()\n",
    "        if not save:\n",
    "            with self.__tboard_validation.as_default():\n",
    "                tf.summary.image(name, data=result, step=stepid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashids import Hashids\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tboard = TBoardGraphsTorch(logname='asd', data_path='/home/hendrik/Documents/master_project/LokalData/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.policies import *\n",
    "policy = ActorCriticPolicy(observation_space=toy_env.observation_space, action_space=toy_env.action_space, lr_schedule=lambda _: torch.finfo(torch.float32).max, net_arch = [dict(pi=[200, 200], vf=[200, 200])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = bc.BC(\n",
    "    observation_space=toy_env.observation_space,\n",
    "    action_space=toy_env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    policy=policy,\n",
    "    device='cpu'\n",
    ")\n",
    "#bc_trainer.train(n_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from active_critic.utils.gym_utils import make_policy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.policy.active_critic_policy import ActiveCriticPolicy\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from metaworld.envs import \\\n",
    "    ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from metaworld.policies import *\n",
    "from gym.wrappers import TimeLimit\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from active_critic.utils.rollout import rollout, make_sample_until, flatten_trajectories\n",
    "from stable_baselines3.common.type_aliases import GymEnv\n",
    "from gym import Env\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "class reset_counter(gym.Wrapper):\n",
    "    def __init__(self, env: Env) -> None:\n",
    "        super().__init__(env)\n",
    "        self.reset_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.reset_count+=1\n",
    "        return super().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PositionalEncoding(d_model=14, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetCounterWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: Env) -> None:\n",
    "        super().__init__(env)\n",
    "        self.reset_count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.reset_count+=1\n",
    "        return super().reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        obsv, rew, done, info = super().step(action)\n",
    "        return obsv, rew, done, info\n",
    "    \n",
    "class ImitationLearningWrapper:\n",
    "    def __init__(self, policy, env: GymEnv):\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        self.policy = policy\n",
    "\n",
    "    def predict(self, obsv, deterministic=None):\n",
    "        actions = []\n",
    "        for obs in obsv:\n",
    "            actions.append(self.policy.get_action(obs))\n",
    "        return actions\n",
    "    \n",
    "class NoLookupWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: Env, seq_len) -> None:\n",
    "        super().__init__(env)\n",
    "        self.obsv = None\n",
    "        self.seq_len = seq_len\n",
    "        self.current_step = 0\n",
    "        self.pe = None\n",
    "        self.succes = 0\n",
    "        self.not_grade = False\n",
    "        self.current_obsv = None\n",
    "\n",
    "    def reset(self):\n",
    "        obsv = super().reset()\n",
    "\n",
    "        if self.pe is None:\n",
    "            if (obsv.shape[-1] % 2 ) != 0:\n",
    "                self.not_grade = True\n",
    "                d_model = obsv.shape[-1] + 1\n",
    "            else:\n",
    "                d_model = obsv.shape[-1]\n",
    "            self.act_d_model = obsv.shape[-1]\n",
    "            self.pe = PositionalEncoding(d_model=d_model, dropout=0)\n",
    "        if self.not_grade:\n",
    "            n_obsv = np.concatenate((obsv, np.zeros_like(obsv)[...,:1]), axis=-1)\n",
    "        else:\n",
    "            n_obsv = obsv\n",
    "        self.obsv = th.tensor(n_obsv)\n",
    "        self.obsv = self.obsv.reshape([1, 1, -1]).repeat([1, self.seq_len, 1])\n",
    "        self.obsv = self.pe.forward(self.obsv)\n",
    "        self.current_obsv = obsv.reshape([1,-1])\n",
    "        return self.obsv[0, 0, :self.act_d_model].numpy()\n",
    "\n",
    "    def step(self, action):\n",
    "        print(action)\n",
    "        obsv, rew, done, info = super().step(action)\n",
    "        self.current_step += 1\n",
    "        if rew == 10:\n",
    "            self.succes = 10\n",
    "        self.current_obsv = obsv.reshape([1,-1])\n",
    "        #return self.obsv[0, self.current_step, :self.act_d_model].numpy(), self.succes, done, info\n",
    "        return obsv, self.succes, done, info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dummy_vec_env(name, seq_len):\n",
    "    policy_dict = make_policy_dict()\n",
    "\n",
    "    env_tag = name\n",
    "    max_episode_steps = seq_len\n",
    "    env = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_tag][1]]()\n",
    "    env._freeze_rand_vec = False\n",
    "    reset_env = ResetCounterWrapper(env=env)\n",
    "    timelimit = TimeLimit(env=reset_env, max_episode_steps=max_episode_steps)\n",
    "    #sparse_lookup = NoLookupWrapper(timelimit, seq_len=seq_len)\n",
    "\n",
    "    dv1 = DummyVecEnv([lambda: RolloutInfoWrapper(timelimit)])\n",
    "    vec_expert = ImitationLearningWrapper(\n",
    "        policy=policy_dict[env_tag][0], env=dv1)\n",
    "    return dv1, vec_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env, expert = make_dummy_vec_env(name='pickplace', seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data import rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions(venv, expert):\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        venv,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=20),\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = sample_expert_transitions(venv=val_env, expert=expert.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_env.step(np.array([[0,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions, rollouts = sample_expert_transitions(vec_expert.predict, env=env, num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(500):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=val_env.observation_space,\n",
    "    action_space=val_env.action_space,\n",
    "    demonstrations=transitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = TBoardGraphs(logname='BC pickplace 10', data_path='/data/bing/hendrik/gboard/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100000):\n",
    "    succ, rew = asd(val_env, bc_trainer.policy)\n",
    "    print(f'succ: {succ.mean()}')\n",
    "    tboard.addValidationScalar('Success Rate', th.tensor(succ.mean()), stepid=i)\n",
    "    bc_trainer.train(n_epochs=1, log_interval=100000000000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TQC\n",
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqc = TQC.load('/home/hendrik/Documents/master_project/LokalData/push_tqc.zip')\n",
    "val_env, _ = make_dummy_vec_env(name='push', seq_len=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sucess, rew = asd(val_env, tqc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sucess[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "a = th.rand([4,3,1])\n",
    "b = a.squeeze().max(dim=-1).values\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TQC\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, n_quantiles=25)\n",
    "learner = TQC(\"MlpPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.use_alt_policy = False\n",
    "learner.alternative_policy = vec_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.tboard_graphs import TBoardGraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_net = BasicRewardNet(\n",
    "    env.observation_space, env.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n",
    "\n",
    "gail_trainer = gail.GAIL(\n",
    "    demonstrations=transitions,\n",
    "    demo_batch_size=1024,\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=env,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.envs[0].reset_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gail_trainer.set_demonstrations(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    gail_trainer.train(1000)\n",
    "    _, next_rollouts = sample_expert_transitions(vec_expert.predict, env=env, num=100)\n",
    "    rollouts = rollouts + next_rollouts\n",
    "    transitions = rollout.flatten_trajectories(rollouts)\n",
    "    gail_trainer.set_demonstrations(transitions)\n",
    "    _, rews = asd(env=env, learner=learner)\n",
    "    print(f'rews: {rews.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rews = asd(env=env, learner=learner)\n",
    "print(f'rews: {rews.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.envs[0].reset_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rews).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rews).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gail_Trainer = gail.GAIL(demonstrations=transitions, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = TBoardGraphsTorch(logname='asd', data_path='/home/hendrik/Documents/master_project/LokalData/test/')\n",
    "for i in range(100):\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = bc_trainer.policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='imitation baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='imitation baseline', opt_gen_trj = None, window=None)\n",
    "    bc_trainer.train(n_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "vec_toy_env = make_vec_env(my_env, n_envs=1)from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n",
    "\n",
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.save('backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.load('backup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLED_ENVS = 0\n",
    "STEPS_TAKEN = 0\n",
    "rein_model = PPO(\"MlpPolicy\", vec_toy_env, verbose=0, learning_rate=1e-4)\n",
    "rein_model.policy = policy\n",
    "rein_model.policy.to(rein_model.device)\n",
    "\n",
    "for i in range(100):\n",
    "    rein_model.learn(total_timesteps=1000)\n",
    "    print_reward(rein_model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLED_ENVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_TAKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_reward(policy, logname = 'ppo'):\n",
    "    tboard = TBoardGraphsTorch(logname=logname, data_path='/home/hendrik/Documents/master_project/LokalData/stableBaselines/')\n",
    "    rew = []\n",
    "    for j in range(1000):from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n",
    "\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(f'num_envs: {SAMPLED_ENVS}')\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='ppo fine tuning baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='ppo fine tuning baseline', opt_gen_trj = None, window=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    rew = []\n",
    "    for j in range(1000):\n",
    "        obs = val_env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = bc_trainer.policy.predict(obs)\n",
    "            obs, reward, done, _ = val_env.step(action=action)\n",
    "        rew.append(reward)\n",
    "    reward = torch.tensor(rew).type(torch.float).mean()\n",
    "    print(reward)\n",
    "    tboard.addValidationScalar('success rate', reward.detach(), stepid=i)\n",
    "    target_trj = val_env.label[val_env.current_env]\n",
    "    gen_trj = val_env.traj\n",
    "    inpt = val_env.data[val_env.current_env][0]\n",
    "\n",
    "    tol_neg = -0.55*torch.ones([val_env.traj.size(-1)])\n",
    "    tol_pos = 0.7*torch.ones([val_env.traj.size(-1)])\n",
    "    tboard.plotDMPTrajectory(target_trj, gen_trj, torch.zeros_like(gen_trj),\n",
    "                                None, None, None, stepid=i, save=False, name_plot='imitation baseline', path='',\\\n",
    "                                    tol_neg=tol_neg, tol_pos=tol_pos, inpt = inpt, name='imitation baseline', opt_gen_trj = None, window=None)\n",
    "    bc_trainer.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.stats\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvergenceDetector():\n",
    "    def __init__(self, model_setup, model_constructor) -> None:\n",
    "        self.model_constructor = model_constructor\n",
    "        self.loss_history = None\n",
    "        self.mean_loss_history = []\n",
    "\n",
    "    def add_loss(self, loss, setup, set_meta):\n",
    "        if self.loss_history is None:\n",
    "            self.loss_history = loss\n",
    "        else:\n",
    "            self.loss_history = torch.cat((self.loss_history, loss))\n",
    "        if self.detect_convergence():\n",
    "            setup()\n",
    "            set_meta()\n",
    "\n",
    "    def torch_compute_confidence_interval(self, data: Tensor,\n",
    "                                           confidence: float = 0.95\n",
    "                                           ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Computes the confidence interval for a given survey of a data set.\n",
    "        \"\"\"\n",
    "        n = len(data)\n",
    "        mean: Tensor = data.mean()\n",
    "        # se: Tensor = scipy.stats.sem(data)  # compute standard error\n",
    "        # se, mean: Tensor = torch.std_mean(data, unbiased=True)  # compute standard error\n",
    "        se: Tensor = data.std(unbiased=True) / (n**0.5)\n",
    "        t_p: float = float(scipy.stats.t.ppf((1 + confidence) / 2., n - 1))\n",
    "        ci = t_p * se\n",
    "        return mean, ci\n",
    "\n",
    "    def detect_convergence(self):\n",
    "        self.mean_loss_history.append(self.torch_compute_confidence_interval(data=self.loss_history))\n",
    "        if len(self.mean_loss_history) > 1:\n",
    "            converged = True\n",
    "            for mean_loss in self.mean_loss_history[-5:-1]:\n",
    "                if (mean_loss[0] > (self.mean_loss_history[-1][0] + self.mean_loss_history[-1][1])):\n",
    "                    converged = False\n",
    "            return converged\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = ConvergenceDetector(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.loss_history = torch.randn(1000)\n",
    "print(cd.detect_convergence())\n",
    "print(cd.detect_convergence())\n",
    "cd.loss_history = torch.randn(1000) - 1\n",
    "print('_______________')\n",
    "print(cd.detect_convergence())\n",
    "print(cd.mean_loss_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.mean_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, net_arch=[512,512,512])\n",
    "learner = TQC(\"MlpPolicy\", env, verbose=1, policy_kwargs=policy_kwargs, buffer_size=1000000, batch_size=2048, gamma=0.95, learning_rate=1e-3, tau=0.05)\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "for i in range(10000):\n",
    "    learner.learn(1000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(success.mean()), stepid=env.envs[0].reset_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct  7 2022, 20:19:58) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
