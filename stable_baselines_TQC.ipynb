{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_critic.utils.gym_utils import make_vec_env, make_dummy_vec_env\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "import torch\n",
    "import numpy as np\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMWToy\n",
    "import warnings\n",
    "from typing import Any, Dict, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from gym import spaces\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
    "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
    "from stable_baselines3.common.utils import explained_variance,  get_schedule_fn\n",
    "\n",
    "from searchTest.toyEnvironment import check_outpt\n",
    "from utilsMW.dataLoaderMW import TorchDatasetMW\n",
    "from torch.utils.data import DataLoader\n",
    "from imitation.algorithms.adversarial import gail \n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from active_critic.utils.tboard_graphs import TBoardGraphs\n",
    "def sample_expert_transitions(expert, env, num):\n",
    "\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        env,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=num),\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts), rollouts\n",
    "\n",
    "def asd(env, learner):\n",
    "    success = []\n",
    "    rews = []\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = learner.predict(obs)\n",
    "            obs, rew, done, info = env.step(action)\n",
    "            rews.append(rew)\n",
    "            if info[0]['success'] > 0:\n",
    "                success.append(info[0]['success'])\n",
    "                break\n",
    "            if done:\n",
    "                success.append(0)\n",
    "    return np.array(success), np.array(rews)\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "env, vec_expert = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "val_env, _ = make_dummy_vec_env(name='pickplace', seq_len=200)\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, n_quantiles=25)\n",
    "learner = TQC(\"MlpPolicy\", env, top_quantiles_to_drop_per_net=2, verbose=1, policy_kwargs=policy_kwargs)\n",
    "\n",
    "tboard = TBoardGraphs(logname='TQC with expert Data', data_path='/data/bing/hendrik/gboard/')\n",
    "learner.use_alt_policy = True\n",
    "learner.alternative_policy = vec_expert\n",
    "best_success = -1\n",
    "for i in range(10000):\n",
    "    learner.learn(5000, log_interval=1000)\n",
    "    success, rews = asd(env=val_env, learner=learner)\n",
    "    tboard.addTrainScalar('Reward', value=th.tensor(rews.mean()), stepid=env.envs[0].reset_count)\n",
    "    mean_success = success.mean()\n",
    "    tboard.addTrainScalar('Success Rate', value=th.tensor(mean_success), stepid=env.envs[0].reset_count)\n",
    "    print(env.envs[0].reset_count)\n",
    "    print(f'Success Rate: {mean_success}')\n",
    "    if mean_success > best_success:\n",
    "        learner.save('/data/bing/hendrik/reach_tqc')\n",
    "        learner.save_replay_buffer('/databing/hendrik/reach_tqc_rpb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hendrik/anaconda3/envs/ac/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sb3_contrib import TQC\n",
    "\n",
    "tqc = TQC.load('/data/bing/hendrik/reatch_tqc.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqc.load_replay_buffer('/data/bing/hendrik/reach_tqc_rpb.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c27e8fa6375f4d15af5a5f5541d8bb88746b588c9fe1102cfd8de011d36c10c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
