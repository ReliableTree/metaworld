{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetaWorld.utilsMW.trainer import ActiveCritic, ActiveCriticArgs\n",
    "from stable_baselines3.common.torch_layers import CombinedExtractor\n",
    "from MetaWorld.utilsMW.model_setup_obj import NetworkSetup\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gym\n",
    "from MetaWorld.searchTest.utils import MyEnv, ToyExpertModel, sample_expert_transitions, benchmark_policy, LearnWrapper, train_policy, LearnWrapper, parse_sampled_transitions, VecExtractor\n",
    "import torch\n",
    "import zipfile\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import importlib\n",
    "import numpy as np\n",
    "from RlBaselines3Zoo import enjoy\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy, ActorCriticPolicy, BaseModel\n",
    "from stable_baselines3.common.torch_layers import (\n",
    "    BaseFeaturesExtractor,\n",
    "    CombinedExtractor,\n",
    "    FlattenExtractor,\n",
    "    NatureCNN,\n",
    "    create_mlp,\n",
    "    get_actor_critic_arch,\n",
    ")\n",
    "from sb3_contrib.tqc.tqc import TQC\n",
    "from sb3_contrib.tqc.policies import MultiInputPolicy\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import os\n",
    "\n",
    "from MetaWorld.searchTest.utils import train_policy\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "model = enjoy.main(inpt_args=\"--algo tqc --env FetchPickAndPlace-v1 --folder /home/hendrik/Documents/master_project/Code/RlBaselines3Zoo/rl-trained-agents -n 300 --ret_model True\")\n",
    "def sample_expert_transitions():\n",
    "    expert = model\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(model.env.envs[0])]),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=100),\n",
    "        unwrap=True,\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "tp = '/home/hendrik/Documents/master_project/LokalData/ImitationLearning/transitions_rew_100'\n",
    "\n",
    "#transitions = sample_expert_transitions()\n",
    "#torch.save(transitions, tp)\n",
    "transitions = torch.load('/home/hendrik/Documents/master_project/LokalData/ImitationLearning/transitions_rew_100')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetaWorld.metaworld.policies.sawyer_pick_place_v2_policy import SawyerPickPlaceV2Policy\n",
    "from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from MetaWorld.utilsMW.makeTrainingData import make_policy_dict\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from typing import Union, Type, Optional\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "from stable_baselines3.common.type_aliases import GymEnv\n",
    "policy_dict = make_policy_dict()\n",
    "env_tag = 'reach'\n",
    "max_episode_steps = 5\n",
    "gt_policy = policy_dict[env_tag]\n",
    "pape = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_tag][1]]()\n",
    "pape._freeze_rand_vec = False\n",
    "timelimit = TimeLimit(env=pape, max_episode_steps=max_episode_steps)\n",
    "dv1 = DummyVecEnv([lambda: RolloutInfoWrapper(timelimit)])\n",
    "class ImitationLearningWrapper:\n",
    "    def __init__(self, policy, env:GymEnv):\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        self.policy = policy\n",
    "\n",
    "    def predict(self, obsv, deterministic=None):\n",
    "        actions = []\n",
    "        for obs in obsv:\n",
    "            actions.append(self.policy.get_action(obs))\n",
    "        return actions\n",
    "IGTP = ImitationLearningWrapper(policy=gt_policy[0], env=dv1).predict\n",
    "\n",
    "def new_epoch(current_obs, check_obsvs):\n",
    "    result =  not th.equal(current_obs.reshape(-1)[-3:], check_obsvs.reshape(-1)[-3:])\n",
    "    return result\n",
    "    \n",
    "class DummyExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, features):\n",
    "        if type(features) is np.ndarray:\n",
    "            features = th.tensor(features)\n",
    "        return features\n",
    "\n",
    "def sample_expert_transitions(expert):\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        dv1,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=5),\n",
    "        unwrap=True,\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n",
    "transitions = sample_expert_transitions(IGTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchTest.utils import make_counter_embedding\n",
    "from searchTest.utils import get_num_bits\n",
    "\n",
    "\n",
    "make_counter_embedding(torch.tensor([4, 1]), 3)\n",
    "get_num_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_transitions(transitions):\n",
    "    observations = np.array(transitions.obs)\n",
    "\n",
    "def strip_obs(obs):\n",
    "    proto_obs = obs[0]\n",
    "    num_bits = get_num_bits(len(obs))\n",
    "    counter_embedding = make_counter_embedding(torch.arange(len(obs)), num_bits)\n",
    "    counter_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sampled_transitions(transitions, new_epoch, extractor):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    infos =[]\n",
    "    next_observations = []\n",
    "    dones = []\n",
    "    rewards = []\n",
    "    epch_actions = []\n",
    "    epch_observations = []\n",
    "    check_obsvs = extractor.forward(transitions[0]['obs'])\n",
    "    for i in range(len(transitions)):\n",
    "        current_obs = extractor.forward(features=transitions[i]['obs'])\n",
    "        next_observation = extractor.forward(features=transitions[i]['next_obs'])\n",
    "\n",
    "        if new_epoch(current_obs, check_obsvs):\n",
    "            rewards.append(epch_success)\n",
    "            check_obsvs = current_obs\n",
    "            observations.append(torch.cat([*epch_observations], dim=0).unsqueeze(0))\n",
    "            actions.append(epch_actions)\n",
    "            next_observations.append(torch.cat([*epch_next_observations], dim=0).unsqueeze(0))\n",
    "            dones.append(epch_dones)\n",
    "            infos.append(epch_infos)\n",
    "\n",
    "            epch_actions = []\n",
    "            epch_observations = []\n",
    "            epch_next_observations = []\n",
    "            epch_dones = []\n",
    "            epch_infos = []\n",
    "        \n",
    "        infos = transitions[i]['infos']\n",
    "        if 'is_success' in infos:\n",
    "            success = transitions[i]['infos']['is_success']\n",
    "        elif 'success' in infos:\n",
    "            success = transitions[i]['infos']['success']\n",
    "\n",
    "        epch_actions.append(transitions[i]['acts'])\n",
    "        epch_observations.append(current_obs.unsqueeze(0))\n",
    "        \n",
    "    observations.append(torch.cat([*epch_observations], dim=0).unsqueeze(0))\n",
    "    actions.append(epch_actions)\n",
    "    rewards.append(success)\n",
    "\n",
    "\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.float)\n",
    "    observations = torch.cat([*observations], dim=0).type(torch.float)\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float)\n",
    "    return actions, observations, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pape = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_tag][1]]()\n",
    "expert = policy_dict[env_tag][0]\n",
    "success = False\n",
    "obsv = pape.reset()\n",
    "i = 0\n",
    "while not success:\n",
    "    act = expert.get_action(obsv)\n",
    "    obsv, _, _, info = pape.step(act)\n",
    "    success = info['success']\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_obj = ActiveCriticArgs()\n",
    "args_obj.set_batchsize(32)\n",
    "args_obj.set_data_path(path='/home/hendrik/Documents/master_project/LokalData/')\n",
    "args_obj.set_device(device='cuda')\n",
    "args_obj.set_feature_extractor(DummyExtractor())\n",
    "args_obj.set_log_name('reach mitation')\n",
    "args_obj.set_meta_optimizer_lr(1e-1)\n",
    "args_obj.set_mlr(5e-5)\n",
    "args_obj.set_lr(5e-5)\n",
    "args_obj.set_network_setup(NetworkSetup())\n",
    "args_obj.set_tboard(True)\n",
    "args_obj.set_demonstrations(demonstrations=transitions)\n",
    "args_obj.set_n_steps(n_steps=10000)\n",
    "args_obj.set_epoch_len(epoch_len=max_episode_steps)\n",
    "args_obj.set_imitation_phase(True)\n",
    "args_obj.set_weight_decay(1e-2)\n",
    "args_obj.set_new_epoch(new_epoch=new_epoch)\n",
    "args_obj.set_eval_epochs(epochs=5)\n",
    "args_obj.set_opt_steps(2)\n",
    "args_obj.set_complete_modulo(1)\n",
    "args_obj.set_observable(True)\n",
    "\n",
    "ac = ActiveCritic(\n",
    "    policy=None,\n",
    "    env=dv1,\n",
    "    args_obj=args_obj,\n",
    "    learning_rate=5e-5,\n",
    "    extractor=DummyExtractor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(weight_decay=1e-1)\n",
    "a = torch.optim.AdamW([torch.ones([3], requires_grad=True)], lr=1, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ac.network.loadNetworkFromFile(path='/home/hendrik/Documents/master_project/LokalData/Data/Model/GT pickplace V2 25/last/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy(trainer=ac, \n",
    "learn_fct=ac.learn, \n",
    "val_env=ac.env, \n",
    "logname=args_obj.logname, \n",
    "path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/',\n",
    "n_epochs=1000,\n",
    "n_steps=1,\n",
    "eval_epochs=1,\n",
    "step_fct=lambda i:i+1,\n",
    "new_epoch=new_epoch,\n",
    "extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = ac.network.sample_new_episode(policy=ac.policy, env=dv1, episodes=5, add_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, t = benchmark_policy(\n",
    "    policy=ac.policy, \n",
    "    path='/home/hendrik/Documents/master_project/LokalData/Data/Model/GT pickplace V2 25/',\n",
    "    logname='reload test',\n",
    "    eval_epochs=20,\n",
    "    val_env=dv1,\n",
    "    stepid=1,\n",
    "    best_reward= -10,\n",
    "    new_epoch=new_epoch,\n",
    "    extractor=DummyExtractor(),\n",
    "    save_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = sample_expert_transitions(ac.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = parse_sampled_transitions(transitions=trans, new_epoch=new_epoch, extractor=DummyExtractor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[400, 300]))\n",
    "# Create the agent\n",
    "model = SAC(\"MlpPolicy\", timelimit, verbose=1, policy_kwargs=policy_kwargs)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_policy = ActorCriticPolicy(observation_space=timelimit.observation_space, action_space=timelimit.action_space, lr_schedule=lambda a:1, net_arch=[512,512,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dv1\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    policy=bc_policy,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy(\n",
    "    trainer=bc_trainer,\n",
    "    learn_fct=bc_trainer.train,\n",
    "    val_env=dv1,\n",
    "    logname='GT 10 V2 ActorCritic',\n",
    "    path='/home/hendrik/Documents/master_project/LokalData/ImitationLearning/',\n",
    "    n_epochs=1000,\n",
    "    n_steps=50,\n",
    "    eval_epochs=100,\n",
    "    step_fct=lambda i: i+1,\n",
    "    new_epoch=new_epoch,\n",
    "    extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_policy(\n",
    "    bc_policy, \n",
    "    path='/home/hendrik/Documents/master_project/LokalData/ImitationLearning/',\n",
    "    logname='BC test gt pick place V2',\n",
    "    eval_epochs=300,\n",
    "    val_env=dv1,\n",
    "    stepid=1,\n",
    "    best_reward=-1,\n",
    "    new_epoch=new_epoch,\n",
    "    extractor=DummyExtractor(),\n",
    "    save_model=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ac.policy.load_state_dict(torch.load('/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/First integrated test/best_modeltensor(-0.9200)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy(trainer=ac, \n",
    "learn_fct=ac.learn, \n",
    "val_env=ac.env, \n",
    "logname='GT pick place V2', \n",
    "path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/',\n",
    "n_epochs=1000,\n",
    "n_steps=1,\n",
    "eval_epochs=1,\n",
    "step_fct=lambda i:i+1,\n",
    "new_epoch=new_epoch,\n",
    "extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.learn(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.network.runValidation(quick=False, pnt=True, epoch=1, save=False, complete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bee90e249730b85f00f3915f0cf4f21bc0729131dcc7008c941068256fd0d344"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tfTest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
