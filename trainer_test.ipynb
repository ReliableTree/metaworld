{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetaWorld.utilsMW.trainer import ActiveCritic, ActiveCriticArgs\n",
    "from stable_baselines3.common.torch_layers import CombinedExtractor\n",
    "from MetaWorld.utilsMW.model_setup_obj import NetworkSetup\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gym\n",
    "from MetaWorld.searchTest.utils import MyEnv, ToyExpertModel, sample_expert_transitions, benchmark_policy, LearnWrapper, train_policy, LearnWrapper, parse_sampled_transitions, VecExtractor\n",
    "import torch\n",
    "import zipfile\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import importlib\n",
    "import numpy as np\n",
    "from RlBaselines3Zoo import enjoy\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy, ActorCriticPolicy, BaseModel\n",
    "from stable_baselines3.common.torch_layers import (\n",
    "    BaseFeaturesExtractor,\n",
    "    CombinedExtractor,\n",
    "    FlattenExtractor,\n",
    "    NatureCNN,\n",
    "    create_mlp,\n",
    "    get_actor_critic_arch,\n",
    ")\n",
    "from sb3_contrib.tqc.tqc import TQC\n",
    "from sb3_contrib.tqc.policies import MultiInputPolicy\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import os\n",
    "\n",
    "from MetaWorld.searchTest.utils import train_policy\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "model = enjoy.main(inpt_args=\"--algo tqc --env FetchPickAndPlace-v1 --folder /home/hendrik/Documents/master_project/Code/RlBaselines3Zoo/rl-trained-agents -n 300 --ret_model True\")\n",
    "def sample_expert_transitions():\n",
    "    expert = model\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(model.env.envs[0])]),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=100),\n",
    "        unwrap=True,\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "tp = '/home/hendrik/Documents/master_project/LokalData/ImitationLearning/transitions_rew_100'\n",
    "\n",
    "#transitions = sample_expert_transitions()\n",
    "#torch.save(transitions, tp)\n",
    "transitions = torch.load('/home/hendrik/Documents/master_project/LokalData/ImitationLearning/transitions_rew_100')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetaWorld.metaworld.policies.sawyer_pick_place_v2_policy import SawyerPickPlaceV2Policy\n",
    "from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from MetaWorld.utilsMW.makeTrainingData import make_policy_dict\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from typing import Union, Type, Optional\n",
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "from stable_baselines3.common.type_aliases import GymEnv\n",
    "policy_dict = make_policy_dict()\n",
    "env_tag = 'reach'\n",
    "gt_policy = policy_dict[env_tag]\n",
    "pape = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_tag][1]]()\n",
    "pape._freeze_rand_vec = False\n",
    "timelimit = TimeLimit(env=pape, max_episode_steps=100)\n",
    "dv1 = DummyVecEnv([lambda: RolloutInfoWrapper(timelimit)])\n",
    "class ImitationLearningWrapper:\n",
    "    def __init__(self, policy, env:GymEnv):\n",
    "        self.observation_space = env.observation_space\n",
    "        self.action_space = env.action_space\n",
    "        self.policy = policy\n",
    "\n",
    "    def predict(self, obsv, deterministic=None):\n",
    "        actions = []\n",
    "        for obs in obsv:\n",
    "            actions.append(self.policy.get_action(obs))\n",
    "        return actions\n",
    "IGTP = ImitationLearningWrapper(policy=gt_policy[0], env=dv1).predict\n",
    "\n",
    "def new_epoch(current_obs, check_obsvs):\n",
    "    result =  not th.equal(current_obs.reshape(-1)[-3:], check_obsvs.reshape(-1)[-3:])\n",
    "    return result\n",
    "    \n",
    "class DummyExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, features):\n",
    "        if type(features) is np.ndarray:\n",
    "            features = th.tensor(features)\n",
    "        return features\n",
    "\n",
    "def sample_expert_transitions(expert):\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        dv1,\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=35),\n",
    "        unwrap=True,\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n",
    "transitions = sample_expert_transitions(IGTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.41248391e-02,  8.28943064e-01,  7.94174167e-02,  9.97497431e-01,\n",
       "        3.95213305e-02,  6.06011948e-01,  1.94164496e-02,  2.37514728e-04,\n",
       "       -1.17463699e-04, -9.25089726e-07,  9.99999965e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  3.41071341e-02,  8.28826540e-01,\n",
       "        7.94762767e-02,  9.97484339e-01,  3.95330070e-02,  6.06038685e-01,\n",
       "        1.94168415e-02, -3.91137262e-04,  1.63602007e-04, -1.01851790e-06,\n",
       "        9.99999910e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        3.41275744e-02,  8.21038261e-01,  8.22315766e-02])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions.obs[transitions.dones][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.41424227e-02,  8.29050199e-01,  7.93625262e-02,  9.97510456e-01,\n",
       "        3.95326435e-02,  6.06035954e-01,  1.94159675e-02, -3.24720725e-04,\n",
       "        1.53458892e-04, -8.52650906e-07,  9.99999936e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  3.41248391e-02,  8.28943064e-01,\n",
       "        7.94174167e-02,  9.97497431e-01,  3.95213305e-02,  6.06011948e-01,\n",
       "        1.94164496e-02,  2.37514728e-04, -1.17463699e-04, -9.25089726e-07,\n",
       "        9.99999965e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        3.41275744e-02,  8.21038261e-01,  8.22315766e-02])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions.next_obs[transitions.dones][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from searchTest.utils import make_counter_embedding\n",
    "from searchTest.utils import get_num_bits\n",
    "\n",
    "\n",
    "make_counter_embedding(torch.tensor([4, 1]), 3)\n",
    "get_num_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_transitions(transitions:types.Transitions):\n",
    "    observations = np.array(transitions.obs)\n",
    "\n",
    "def strip_obs(obs):\n",
    "    proto_obs = obs[0]\n",
    "    num_bits = get_num_bits(len(obs))\n",
    "    counter_embedding = make_counter_embedding(torch.arange(len(obs)), num_bits)\n",
    "    counter_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sampled_transitions(transitions, new_epoch, extractor):\n",
    "    observations = []\n",
    "    actions = []\n",
    "    infos =[]\n",
    "    next_observations = []\n",
    "    dones = []\n",
    "    rewards = []\n",
    "    epch_actions = []\n",
    "    epch_observations = []\n",
    "    check_obsvs = extractor.forward(transitions[0]['obs'])\n",
    "    for i in range(len(transitions)):\n",
    "        current_obs = extractor.forward(features=transitions[i]['obs'])\n",
    "        next_observation = extractor.forward(features=transitions[i]['next_obs'])\n",
    "\n",
    "        if new_epoch(current_obs, check_obsvs):\n",
    "            rewards.append(epch_success)\n",
    "            check_obsvs = current_obs\n",
    "            observations.append(torch.cat([*epch_observations], dim=0).unsqueeze(0))\n",
    "            actions.append(epch_actions)\n",
    "            next_observations.append(torch.cat([*epch_next_observations], dim=0).unsqueeze(0))\n",
    "            dones.append(epch_dones)\n",
    "            infos.append(epch_infos)\n",
    "\n",
    "            epch_actions = []\n",
    "            epch_observations = []\n",
    "            epch_next_observations = []\n",
    "            epch_dones = []\n",
    "            epch_infos = []\n",
    "        \n",
    "        infos = transitions[i]['infos']\n",
    "        if 'is_success' in infos:\n",
    "            success = transitions[i]['infos']['is_success']\n",
    "        elif 'success' in infos:\n",
    "            success = transitions[i]['infos']['success']\n",
    "\n",
    "        epch_actions.append(transitions[i]['acts'])\n",
    "        epch_observations.append(current_obs.unsqueeze(0))\n",
    "        \n",
    "    observations.append(torch.cat([*epch_observations], dim=0).unsqueeze(0))\n",
    "    actions.append(epch_actions)\n",
    "    rewards.append(success)\n",
    "\n",
    "\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.float)\n",
    "    observations = torch.cat([*observations], dim=0).type(torch.float)\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float)\n",
    "    return actions, observations, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hendrik/Documents/master_project/Code/MetaWorld/trainer_test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/trainer_test.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m actions, observations, rewards \u001b[39m=\u001b[39m parse_sampled_transitions(transitions, new_epoch\u001b[39m=\u001b[39;49mnew_epoch, extractor\u001b[39m=\u001b[39;49mDummyExtractor())\n",
      "\u001b[1;32m/home/hendrik/Documents/master_project/Code/MetaWorld/trainer_test.ipynb Cell 4\u001b[0m in \u001b[0;36mparse_sampled_transitions\u001b[0;34m(transitions, new_epoch, extractor)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/trainer_test.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m current_obs \u001b[39m=\u001b[39m extractor\u001b[39m.\u001b[39mforward(features\u001b[39m=\u001b[39mtransitions[i][\u001b[39m'\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/trainer_test.ipynb#X54sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m new_epoch(current_obs, check_obsvs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/trainer_test.ipynb#X54sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39massert\u001b[39;00m transitions[i][\u001b[39m'\u001b[39m\u001b[39mdones\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/trainer_test.ipynb#X54sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     rewards\u001b[39m.\u001b[39mappend(success)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hendrik/Documents/master_project/Code/MetaWorld/trainer_test.ipynb#X54sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     check_obsvs \u001b[39m=\u001b[39m current_obs\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "actions, observations, rewards = parse_sampled_transitions(transitions, new_epoch=new_epoch, extractor=DummyExtractor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 100, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data import types\n",
    "new_ = {}\n",
    "for ele in transitions[0]:\n",
    "    if ele in ['obs', 'next_obs', 'acts']:\n",
    "        new_[ele] = np.expand_dims(transitions[0][ele], 0)\n",
    "    else:\n",
    "        new_[ele] = np.array([transitions[0][ele]])\n",
    "new_\n",
    "test = types.Transitions(**new_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs': array([[ 6.15235164e-03,  6.00189803e-01,  1.94301175e-01,\n",
       "          1.00000000e+00, -3.69143304e-02,  6.36371088e-01,\n",
       "          1.99999996e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          3.41424227e-02,  8.29050199e-01,  7.93625262e-02,\n",
       "          9.97510456e-01,  3.95326435e-02,  6.06035954e-01,\n",
       "          1.94159675e-02, -3.24720725e-04,  1.53458892e-04,\n",
       "         -8.52650906e-07,  9.99999936e-01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         -1.22796975e-02,  8.98837360e-01,  7.55112046e-02]]),\n",
       " 'acts': array([[-0.09216025,  1.49323797, -0.59394985,  0.        ]]),\n",
       " 'infos': array([{'success': 0.0, 'near_object': 0.3076087313338501, 'grasp_success': 1.0, 'grasp_reward': 0.3076087313338501, 'in_place_reward': 0.14945909143580222, 'obj_to_target': 0.3076087313338501, 'unscaled_reward': 1.4945909143580223}],\n",
       "       dtype=object),\n",
       " 'next_obs': array([[ 5.74836592e-03,  6.00395352e-01,  1.93782533e-01,\n",
       "          1.00000000e+00, -3.69133961e-02,  6.36370048e-01,\n",
       "          1.99227042e-02,  2.79614491e-05,  2.21267421e-05,\n",
       "          7.07185861e-09,  9.99999999e-01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          6.15235164e-03,  6.00189803e-01,  1.94301175e-01,\n",
       "          1.00000000e+00, -3.69143304e-02,  6.36371088e-01,\n",
       "          1.99999996e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         -1.22796975e-02,  8.98837360e-01,  7.55112046e-02]]),\n",
       " 'dones': array([False])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert = policy_dict[env_tag][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pape = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_tag][1]]()\n",
    "expert = policy_dict[env_tag][0]\n",
    "success = False\n",
    "obsv = pape.reset()\n",
    "i = 0\n",
    "while not success:\n",
    "    act = expert.get_action(obsv)\n",
    "    obsv, _, _, info = pape.step(act)\n",
    "    success = info['success']\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    act = np.array([0.5,0.5,0.5,0.5])\n",
    "    obsv, _, _, info1 = pape.step(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pape = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict[env_tag][1]]()\n",
    "res = pape.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv1, _, _, info1 = pape.step(np.array([-0.5,-0.5,-0.5,-0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res-obsv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, observations, rewards = parse_sampled_transitions(transitions, new_epoch=new_epoch, extractor=DummyExtractor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transitions = th.load('tmp/imitation/transitions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_obj = ActiveCriticArgs()\n",
    "args_obj.set_batchsize(32)\n",
    "args_obj.set_data_path(path='/home/hendrik/Documents/master_project/LokalData/')\n",
    "args_obj.set_device(device='cuda')\n",
    "args_obj.set_feature_extractor(DummyExtractor())\n",
    "args_obj.set_log_name('reach no imitation')\n",
    "args_obj.set_meta_optimizer_lr(1e-1)\n",
    "args_obj.set_mlr(5e-5)\n",
    "args_obj.set_network_setup(NetworkSetup())\n",
    "args_obj.set_tboard(True)\n",
    "args_obj.set_demonstrations(demonstrations=None)\n",
    "args_obj.set_n_steps(n_steps=100)\n",
    "args_obj.set_epoch_len(epoch_len=100)\n",
    "args_obj.set_imitation_phase(False)\n",
    "args_obj.set_weight_decay(1e-2)\n",
    "args_obj.set_new_epoch(new_epoch=new_epoch)\n",
    "args_obj.set_eval_epochs(epochs=5)\n",
    "args_obj.set_opt_steps(150)\n",
    "args_obj.set_complete_modulo(1)\n",
    "\n",
    "ac = ActiveCritic(\n",
    "    policy=None,\n",
    "    env=dv1,\n",
    "    args_obj=args_obj,\n",
    "    learning_rate=5e-5,\n",
    "    extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ac.network.loadNetworkFromFile(path='/home/hendrik/Documents/master_project/LokalData/Data/Model/GT pickplace V2 25/last/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy(trainer=ac, \n",
    "learn_fct=ac.learn, \n",
    "val_env=ac.env, \n",
    "logname=args_obj.logname, \n",
    "path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/',\n",
    "n_epochs=1000,\n",
    "n_steps=1,\n",
    "eval_epochs=1,\n",
    "step_fct=lambda i:i+1,\n",
    "new_epoch=new_epoch,\n",
    "extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = ac.network.sample_new_episode(policy=ac.policy, env=dv1, episodes=5, add_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, t = benchmark_policy(\n",
    "    policy=ac.policy, \n",
    "    path='/home/hendrik/Documents/master_project/LokalData/Data/Model/GT pickplace V2 25/',\n",
    "    logname='reload test',\n",
    "    eval_epochs=20,\n",
    "    val_env=dv1,\n",
    "    stepid=1,\n",
    "    best_reward= -10,\n",
    "    new_epoch=new_epoch,\n",
    "    extractor=DummyExtractor(),\n",
    "    save_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = sample_expert_transitions(ac.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = parse_sampled_transitions(transitions=trans, new_epoch=new_epoch, extractor=DummyExtractor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[400, 300]))\n",
    "# Create the agent\n",
    "model = SAC(\"MlpPolicy\", timelimit, verbose=1, policy_kwargs=policy_kwargs)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_policy = ActorCriticPolicy(observation_space=timelimit.observation_space, action_space=timelimit.action_space, lr_schedule=lambda a:1, net_arch=[512,512,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dv1\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    policy=bc_policy,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy(\n",
    "    trainer=bc_trainer,\n",
    "    learn_fct=bc_trainer.train,\n",
    "    val_env=dv1,\n",
    "    logname='GT 10 V2 ActorCritic',\n",
    "    path='/home/hendrik/Documents/master_project/LokalData/ImitationLearning/',\n",
    "    n_epochs=1000,\n",
    "    n_steps=50,\n",
    "    eval_epochs=100,\n",
    "    step_fct=lambda i: i+1,\n",
    "    new_epoch=new_epoch,\n",
    "    extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_policy(\n",
    "    bc_policy, \n",
    "    path='/home/hendrik/Documents/master_project/LokalData/ImitationLearning/',\n",
    "    logname='BC test gt pick place V2',\n",
    "    eval_epochs=300,\n",
    "    val_env=dv1,\n",
    "    stepid=1,\n",
    "    best_reward=-1,\n",
    "    new_epoch=new_epoch,\n",
    "    extractor=DummyExtractor(),\n",
    "    save_model=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ac.policy.load_state_dict(torch.load('/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/First integrated test/best_modeltensor(-0.9200)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy(trainer=ac, \n",
    "learn_fct=ac.learn, \n",
    "val_env=ac.env, \n",
    "logname='GT pick place V2', \n",
    "path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/',\n",
    "n_epochs=1000,\n",
    "n_steps=1,\n",
    "eval_epochs=1,\n",
    "step_fct=lambda i:i+1,\n",
    "new_epoch=new_epoch,\n",
    "extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.learn(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.network.runValidation(quick=False, pnt=True, epoch=1, save=False, complete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bee90e249730b85f00f3915f0cf4f21bc0729131dcc7008c941068256fd0d344"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tfTest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
