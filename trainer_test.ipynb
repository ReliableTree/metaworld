{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetaWorld.utilsMW.trainer import ActiveCritic, ActiveCriticArgs\n",
    "from stable_baselines3.common.torch_layers import CombinedExtractor\n",
    "from MetaWorld.utilsMW.model_setup_obj import NetworkSetup\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import gym\n",
    "from MetaWorld.searchTest.utils import MyEnv, ToyExpertModel, sample_expert_transitions, benchmark_policy, LearnWrapper, train_policy, LearnWrapper, parse_sampled_transitions, VecExtractor\n",
    "import torch\n",
    "import zipfile\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "import importlib\n",
    "import numpy as np\n",
    "from RlBaselines3Zoo import enjoy\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy, ActorCriticPolicy, BaseModel\n",
    "from stable_baselines3.common.torch_layers import (\n",
    "    BaseFeaturesExtractor,\n",
    "    CombinedExtractor,\n",
    "    FlattenExtractor,\n",
    "    NatureCNN,\n",
    "    create_mlp,\n",
    "    get_actor_critic_arch,\n",
    ")\n",
    "from sb3_contrib.tqc.tqc import TQC\n",
    "from sb3_contrib.tqc.policies import MultiInputPolicy\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import os\n",
    "\n",
    "from MetaWorld.searchTest.utils import train_policy\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "model = enjoy.main(inpt_args=\"--algo tqc --env FetchPickAndPlace-v1 --folder /home/hendrik/Documents/master_project/Code/RlBaselines3Zoo/rl-trained-agents -n 300 --ret_model True\")\n",
    "def sample_expert_transitions():\n",
    "    expert = model\n",
    "\n",
    "    print(\"Sampling expert transitions.\")\n",
    "    rollouts = rollout.rollout(\n",
    "        expert,\n",
    "        DummyVecEnv([lambda: RolloutInfoWrapper(model.env.envs[0])]),\n",
    "        rollout.make_sample_until(min_timesteps=None, min_episodes=100),\n",
    "        unwrap=True,\n",
    "        exclude_infos=False,\n",
    "    )\n",
    "    return rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "tp = '/home/hendrik/Documents/master_project/LokalData/ImitationLearning/transitions_rew_100'\n",
    "\n",
    "#transitions = sample_expert_transitions()\n",
    "#torch.save(transitions, tp)\n",
    "transitions = torch.load('/home/hendrik/Documents/master_project/LokalData/ImitationLearning/transitions_rew_100')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetaWorld.metaworld.policies.sawyer_pick_place_v2_policy import SawyerPickPlaceV2Policy\n",
    "from metaworld.envs import ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE\n",
    "from MetaWorld.utilsMW.makeTrainingData import make_policy_dict\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
    "from typing import Union, Type, Optional\n",
    "from stable_baselines3.common.type_aliases import GymEnv\n",
    "from searchTest.utils import (\n",
    "    ImitationLearningWrapper, \n",
    "    make_vec_env, \n",
    "    make_rollouts_vec, \n",
    "    DummyExtractor, \n",
    "    parse_sampled_transitions, \n",
    "    sample_expert_transitions,\n",
    "    new_epoch,\n",
    "    new_epoch_np\n",
    ")\n",
    "\n",
    "policy_dict = make_policy_dict()\n",
    "gt_policy = policy_dict['pickplace']\n",
    "pape = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict['pickplace'][1]]\n",
    "num_cpu = 1\n",
    "ve1 = make_vec_env(num_cpu=num_cpu, env_constr=pape, epoch_len=100, seed=0)\n",
    "\n",
    "\n",
    "IGTP = ImitationLearningWrapper(policy=gt_policy[0], env=ve1).predict\n",
    "\n",
    "#transitions = sample_expert_transitions(IGTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00615235, 0.6001898 , 0.19430117, 1.        , 0.0097627 ,\n",
       "        0.67151893, 0.02      , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00615235, 0.6001898 ,\n",
       "        0.19430117, 1.        , 0.0097627 , 0.67151893, 0.02      ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00897664, 0.84236548, 0.21147354]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve1.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4203133778]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve1.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00615235, 0.6001898 , 0.19430117, 1.        , 0.0097627 ,\n",
       "        0.67151893, 0.02      , 0.        , 0.        , 0.        ,\n",
       "        1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00615235, 0.6001898 ,\n",
       "        0.19430117, 1.        , 0.0097627 , 0.67151893, 0.02      ,\n",
       "        0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00897664, 0.84236548, 0.21147354]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ve1.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(99):\n",
    "    obsv, rew, dones, info = ve1.step(np.array([[0,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsv, rew, dones, info = ve1.step(np.array([[0,0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.random.randint(0, 1e7, 1)\n",
    "transitions = make_rollouts_vec(seeds=seeds, expert=IGTP, env_constr=pape, num_cpu=4, epoch_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions, observations, rewards = parse_sampled_transitions(transitions, new_epoch=new_epoch, extractor=DummyExtractor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_obj = ActiveCriticArgs()\n",
    "args_obj.set_batchsize(32)\n",
    "args_obj.set_data_path(path='/home/hendrik/Documents/master_project/LokalData/')\n",
    "args_obj.set_device(device='cuda')\n",
    "args_obj.set_feature_extractor(DummyExtractor())\n",
    "args_obj.set_log_name('GT pickplace V2 100 Obsv')\n",
    "args_obj.set_meta_optimizer_lr(1e-2)\n",
    "args_obj.set_mlr(5e-5)\n",
    "args_obj.set_network_setup(NetworkSetup())\n",
    "args_obj.set_tboard(True)\n",
    "args_obj.set_demonstrations(demonstrations=transitions)\n",
    "args_obj.set_n_steps(n_steps=10000)\n",
    "#args_obj.set_n_steps(n_steps=30)\n",
    "args_obj.set_epoch_len(epoch_len=100)\n",
    "args_obj.set_imitation_phase(True)\n",
    "args_obj.set_weight_decay(1e-2)\n",
    "args_obj.set_new_epoch(new_epoch=new_epoch)\n",
    "args_obj.set_eval_epochs(epochs=50)\n",
    "args_obj.set_opt_steps(20)\n",
    "args_obj.set_complete_modulo(1)\n",
    "args_obj.set_num_cpu(10)\n",
    "\n",
    "pape = ALL_V2_ENVIRONMENTS_GOAL_OBSERVABLE[policy_dict['pickplace'][1]]\n",
    "args_obj.set_env_factory(lambda num_cpu, env_constr, epoch_len, seed: make_vec_env(num_cpu=num_cpu, env_constr=env_constr, epoch_len=epoch_len, seed=seed))\n",
    "\n",
    "ac = ActiveCritic(\n",
    "    policy=None,\n",
    "    env=pape,\n",
    "    args_obj=args_obj,\n",
    "    learning_rate=5e-5,\n",
    "    extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ac.network.loadNetworkFromFile(path='/home/hendrik/Documents/master_project/LokalData/Data/Model/GT pickplace V2 25/last/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy(trainer=ac, \n",
    "learn_fct=ac.learn, \n",
    "val_env=ac.env, \n",
    "logname=args_obj.logname, \n",
    "path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/',\n",
    "n_epochs=1000,\n",
    "n_steps=1,\n",
    "eval_epochs=1,\n",
    "step_fct=lambda i:i+1,\n",
    "new_epoch=new_epoch,\n",
    "extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = ac.network.sample_new_episode(policy=ac.policy, env=dv1, episodes=5, add_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, t = benchmark_policy(\n",
    "    policy=ac.policy, \n",
    "    path='/home/hendrik/Documents/master_project/LokalData/Data/Model/GT pickplace V2 25/',\n",
    "    logname='reload test',\n",
    "    eval_epochs=20,\n",
    "    val_env=dv1,\n",
    "    stepid=1,\n",
    "    best_reward= -10,\n",
    "    new_epoch=new_epoch,\n",
    "    extractor=DummyExtractor(),\n",
    "    save_model=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = sample_expert_transitions(ac.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = parse_sampled_transitions(transitions=trans, new_epoch=new_epoch, extractor=DummyExtractor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[400, 300]))\n",
    "# Create the agent\n",
    "model = SAC(\"MlpPolicy\", timelimit, verbose=1, policy_kwargs=policy_kwargs)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_policy = ActorCriticPolicy(observation_space=timelimit.observation_space, action_space=timelimit.action_space, lr_schedule=lambda a:1, net_arch=[512,512,512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = dv1\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    policy=bc_policy,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy(\n",
    "    trainer=bc_trainer,\n",
    "    learn_fct=bc_trainer.train,\n",
    "    val_env=dv1,\n",
    "    logname='GT 10 V2 ActorCritic',\n",
    "    path='/home/hendrik/Documents/master_project/LokalData/ImitationLearning/',\n",
    "    n_epochs=1000,\n",
    "    n_steps=50,\n",
    "    eval_epochs=100,\n",
    "    step_fct=lambda i: i+1,\n",
    "    new_epoch=new_epoch,\n",
    "    extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_policy(\n",
    "    bc_policy, \n",
    "    path='/home/hendrik/Documents/master_project/LokalData/ImitationLearning/',\n",
    "    logname='BC test gt pick place V2',\n",
    "    eval_epochs=300,\n",
    "    val_env=dv1,\n",
    "    stepid=1,\n",
    "    best_reward=-1,\n",
    "    new_epoch=new_epoch,\n",
    "    extractor=DummyExtractor(),\n",
    "    save_model=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ac.policy.load_state_dict(torch.load('/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/First integrated test/best_modeltensor(-0.9200)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_policy(trainer=ac, \n",
    "learn_fct=ac.learn, \n",
    "val_env=ac.env, \n",
    "logname='GT pick place V2', \n",
    "path='/home/hendrik/Documents/master_project/LokalData/TransformerImitationLearning/',\n",
    "n_epochs=1000,\n",
    "n_steps=1,\n",
    "eval_epochs=1,\n",
    "step_fct=lambda i:i+1,\n",
    "new_epoch=new_epoch,\n",
    "extractor=DummyExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.learn(n_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.network.runValidation(quick=False, pnt=True, epoch=1, save=False, complete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bee90e249730b85f00f3915f0cf4f21bc0729131dcc7008c941068256fd0d344"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tfTest')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
